# This is adapted from the config in Andraw Kaparthy's repo: nanoGPT
# It is originally designed for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB
# I/O

dataset_path: '/Users/bangliu/Desktop/ChatSCI/Aeiva/datasets/merge/avsd_alpaca_vqa.json'
dataset_name: 'customized'
is_custom_dataset: False,
customized_cache_dir: '/Users/bangliu/Desktop/ChatSCI/Aeiva/datasets/merge/.cache/'
dataset_config_name: None
model_name_or_path: 'macaw-7b'

output_dir: '/Users/bangliu/Desktop/ChatSCI/Aeiva/outputs/' # output folder name for saving checkpoints
save_checkpoint_every_n_train_steps: 1 # save checkpoint for every n train steps during training
val_check_interval: 1 # evaluate the loss on train/validation set for every val_check_interval train steps
log_every_n_steps: 1
limit_val_batches: 1 # limit the number of batches to use for fast estimate val loss during training
# eval_only: False # if True, script exits right after the first eval
always_save_checkpoint: True # if True, always save a checkpoint after each eval
init_from: 'scratch' # 'scratch' or 'resume' or 'gpt2*'

# wandb logging
use_wandb: True
wandb_project: 'train-macaw-7b'
wandb_run_name: 'train-macaw-7b'

# data
dataset_name: 'macaw'
accumulate_grad_batches: 1 # used to simulate larger batch sizes
batch_size: 2 # if accumulate_grad_batches > 1, this is the micro-batch size

# model
# n_layer: 12
# n_head: 12
# n_embd: 768
# dropout: 0.0 # for pretraining 0 is good, for finetuning try 0.1+
# bias: False # do we use bias inside LayerNorm and Linear layers?

# adamw optimizer
learning_rate: 6.0e-4 # max learning rate # Bang: Note: don't write 1e-1 style, it will be considered as string
max_steps: 2 # total number of training iterations
weight_decay: 1.0e-1
adam_beta1: 0.9
adam_beta2: 0.99
max_grad_norm: 1.0 # clip gradients at this value, or disable if == 0.0

# learning rate decay settings
lr_decay: True # whether to decay the learning rate
warmup_steps: 2 # how many steps to warm up for  # Note, it must be smaller than max_steps
lr_decay_steps: 2 # should be ~= max_iters per Chinchilla. Must be larger than 1.
min_lr: 6.0e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# DDP settings
# ddp_backend: 'nccl' # 'nccl', 'gloo', etc.

# system
device: 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks
torch_dtype: 'float32' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler
torch_compile: True # use PyTorch 2.0 to compile the model to be faster

# macaw custom config
image_dir: '/Users/bangliu/Desktop/ChatSCI/Aeiva/datasets/coco/train2014/'
audio_dir: '/Users/bangliu/Desktop/ChatSCI/Aeiva/datasets/avsd/audios/'
frame_dir: '/Users/bangliu/Desktop/ChatSCI/Aeiva/datasets/avsd/frames/'
video_dir: '/Users/bangliu/Desktop/ChatSCI/Aeiva/datasets/avsd/videos/'
num_frames_to_sample: 120
num_frames_to_load: 6
block_size: 512
num_samples_per_dataset: 100
num_samples_per_merged_dataset: 20
max_seq_len_for_preprocess: 256

tokenizer_name_or_path: '/Users/bangliu/Desktop/ChatSCI/Aeiva/pretrained_models/macaw/'
clip_model_name_or_path: '/Users/bangliu/Desktop/ChatSCI/Aeiva/pretrained_models/clip_model/'
whisper_model_name_or_path: '/Users/bangliu/Desktop/ChatSCI/Aeiva/pretrained_models/whisper_model/'
llama7b_model_name_or_path: '/Users/bangliu/Desktop/ChatSCI/Aeiva/pretrained_models/llama7b_model/'
macaw_model_name_or_path: "/Users/bangliu/Desktop/ChatSCI/Aeiva/pretrained_models/macaw/"

run_time_cache_dir: '/Users/bangliu/Desktop/ChatSCI/Aeiva/outputs/cache/'

mode: "train"