# Roadmap

In this document, we describe the long- and short-term objectives of this project, as well as show the current plan in achieving the objectives.

## Long-term Objective
Enable Aeiva to learn from multimodal and embodied environments with high learning efficiency and low-resource requirements. Specifically, we aim to research on the following aspects:

* Multimodal learning
	* Be able to learn from video data, which is the main modality for human beings.
	* Be able to intergrate different modalities, e.g., text, image, audio, video, documents, and so on.

* Embodied learning
	* Be able to learn from embodied environments, e.g., MineDojo or any other video games, without reading internal states (i.e., by just observing raw visual inputs).
	* Be able to imagine the environment of videos and learn from them.
	* Unify the learning paradigm in different environments and different modalities.

* AI for Science
	* Health
	* Material Science

* Safe, Controllable, Interpretable AI
	* Research on techniques to ensure safe, controllable, and interpretable AI

* Efficient learning
	* Understand how LLMs learn with large-scale data
	* Improve the learning efficiency of language models.
	* Reducing the model size while maintaining the capability of models.
	* The final goal is approaching or even surpassing the learning and energy efficiency of human brain.

* Artificial Neural Science
	* Understand LLMs and other DL models: what they learned, how they learn, and how to improve.
	* Intergrating neural science knowledge to improve DL models.

* Manipulating tools
	* Enable the models to utilize tools, quickly learn new tools, and interact with different environments with tools

* AI Society
	* Evolving a society of AI agents
	* Combining AI agents with virtual environment
	* Agents learn to solve problems in real-world
	* Ensure the human rights

## Short-term objective (keep updating)
The long-term objective of this project is quite ambitious. At the first place, we want to better understand how the current LLMs learn, and improve the multimodal and embodied learning. Specifically, we want to learn from videos efficiently. Below is a list of milestones we aim to achieve in a short-term (keep updating):

* Multimodal Learning
	* Benchmarking several existing multimodal LLMs in a unified manner
	* Making model construction as easy as playing LEGO
	* Unifing different datasets
	* Define the general framework of multimodal learning
* Embodied Learning
	* Integrating several embodied environments, e.g., MineDojo, Alfred, etc.
	* Completing the framework of embodied learning, i.e., equiping LLMs with environments, actions, rewards, goals, and so on.
* AI Agent
	* Design agent and related classes
	* Design memory module
	* Design world model module
	* Design actions, rewards, etc.
	* Implement several learning algorithms.
* AI Society
	* Design AI community and communication protocals
	* Add visualizatioin UI

......