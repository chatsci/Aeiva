{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Aeiva","text":"<p>Home page...</p>"},{"location":"coding_guidelines/","title":"Coding Guidelines","text":""},{"location":"coding_guidelines/#code-hierarchy","title":"Code Hierarchy","text":"<p>Generally, our code can be organized into three different levels:</p> <ol> <li> <p>Framework: This level forms the architectural backbone of your project. It houses the core functionalities that define the basic structure and shared logic for your project. These files, typically stored under the <code>package_name/bases/</code> directory, establish the protocols and high-level operations that the rest of your project will adhere to.</p> </li> <li> <p>Brick: The \"Brick\" level acts as a collection of modular, reusable components used across your project. These components, which are stored in the <code>package_name/xxx/</code> directory, promote code reusability and reduce redundancy, thereby enhancing the efficiency of your codebase.</p> </li> <li> <p>Applications: This level contains specific implementations associated with particular datasets, models, or experiments. These files, which are stored in the <code>package_name/</code> directory, are separate from the abstract base classes and reusable functions found in the other levels. This separation aids in code navigation and readability, making it easier to locate and understand the specific components of your project.</p> </li> </ol> <p>By adhering to this structure, your codebase will be well-organized, easily navigable, and efficient. This organization adheres to best practices in software development, promoting code reusability and a clear separation of concerns.</p>"},{"location":"coding_guidelines/#generate-requirementstxt","title":"Generate requirements.txt","text":"<p>Use pipreqs: pipreqs is a useful tool that generates a requirements.txt file based on the imports in your Python project, not on the installed packages in your current environment. You can install it and use it as follows:</p> <pre><code>pip install pipreqs\npipreqs --force /path/to/your/project\n</code></pre>"},{"location":"coding_guidelines/#args-and-kwargs","title":"args and *kwargs","text":"<p><code>*args</code> and <code>**kwargs</code> in Python allow a function to accept optional arguments, meaning that the user can pass a variable number of arguments to these functions. Here's when you might want to use them:</p> <ol> <li> <p>When you're not sure how many arguments might be passed to your function: <code>*args</code> is used to send a non-keyworded variable-length argument list to your function. You might use it when you're not sure how many arguments might be passed to your function, or if you want to support an arbitrary number of arguments.</p> </li> <li> <p>When you want to write a function that must accept a dictionary: <code>**kwargs</code> is used to pass a keyworded, variable-length argument list. You would use this if you want your function to be able to accept a dictionary of attributes.</p> </li> <li> <p>When creating wrapper functions or decorators: <code>*args</code> and <code>**kwargs</code> are commonly used when you're writing higher-order functions or decorators that need to manipulate the inputs to another function that they're wrapping.</p> </li> <li> <p>When subclassing and you want to extend the parent class's methods: In this case, you may not know exactly what the parent class's method takes as arguments. <code>*args</code> and <code>**kwargs</code> let you pass any parameters from the child class to the parent class's method without having to know what those parameters are.</p> </li> </ol> <p>However, while <code>*args</code> and <code>**kwargs</code> are very helpful, they should be used judiciously. Overuse can make your code harder to understand and debug since it's not immediately clear what arguments a function expects. When writing a function, if you know the exact number and role of each argument, it's better to list them explicitly.</p> <p>In summary, <code>*args</code> and <code>**kwargs</code> are powerful tools that make Python functions more flexible. However, as with any tool, they should be used judiciously and appropriately.</p>"},{"location":"coding_guidelines/#order-of-function-arguments-in-python","title":"Order of Function Arguments in Python","text":"<p>In Python, the recommended order of function parameters is as follows:</p> <ol> <li> <p>Required positional arguments: These are arguments that need to be in a specific positional order. When calling the function, Python interprets them based on their order.</p> <p>Example: <code>def func(name, age):</code></p> </li> <li> <p>Optional positional arguments / Default Parameters: These are arguments that are optional and have a default value. They are also interpreted based on their order.</p> <p>Example: <code>def func(name, age=22):</code></p> </li> <li> <p>Required keyword-only arguments: These are arguments that must be supplied by keyword and follow a \"*,\" in the function definition.</p> <p>Example: <code>def func(name, age, *, city):</code></p> </li> <li> <p>Optional keyword-only arguments / Default Keyword Parameters: These are keyword arguments that are optional. The function will use the default value if no value is provided.</p> <p>Example: <code>def func(name, age, *, city='New York'):</code></p> </li> <li> <p>Arbitrary argument lists: The <code>*args</code> and <code>**kwargs</code> parameters, which collect all positional and keyword arguments that are not already caught by other parameters.</p> <p>Example: <code>def func(name, age, *args, city='New York', **kwargs):</code></p> </li> </ol> <p>This order can help make your function definitions clear and easy to read. It also helps prevent common bugs caused by confusing positional and keyword arguments.</p>"},{"location":"coding_guidelines/#naming-noun-or-verb","title":"Naming: Noun or Verb?","text":"Thing Choice of Word Modules Noun Data types Noun or Adjective Functions Noun or Verb Constants/Variables Noun <ul> <li>Try to make your name short and avoid longer than 3 words name if possible.</li> <li>Use verb or noun for functions or methods depends on what you want to emphasize: the return result or the process to get the result.</li> </ul> <p>To better choose verbs for functions, below are some suggestions:</p> <ol> <li>Is the function a test? -&gt; test_\\_\\. <li> <p>Does the function has a @property decorator? -&gt; don\u2019t use a verb in the function name.</p> </li> <li> <p>Does the function use a disk or a network:</p> <p>3.1. \u2026 to store data? -&gt; save_to, send, write_to</p> <p>3.2. \u2026 to receive data? -&gt; fetch, load, read</p> </li> <li> <p>Does the function output any data? -&gt; print, output</p> </li> <li> <p>Returns boolean value? -&gt; is_, has_/have_, can_, check_if_\\_\\ <li> <p>Aggregates data? -&gt; calculate, extract, analyze</p> </li> <li> <p>Put data from one form to another:</p> <p>7.1. Creates a single meaningful object? -&gt; create</p> <p>7.2. Fills an existing object with data? -&gt; initialize, configure</p> <p>7.3. Clean raw data? -&gt; clean</p> <p>7.4. Receive a string as input? -&gt; parse</p> <p>7.5. Return a string as output? -&gt; render</p> <p>7.6. Return an iterator as output? -&gt;iter</p> <p>7.7. Mutates its arguments or some global state? -&gt; update, mutate, add, remove, insert, set</p> <p>7.8. Return a list of errors? -&gt; validate</p> <p>7.9. Checks data items recursively? -&gt; walk</p> <p>7.10. Finds appropriate item in data? -&gt; find, search, match</p> <p>7.11. Transform data type? -&gt; \\_to_\\ <p>7.12. None of the above, but still works with data? -&gt; Check one of those: morph, compose, prepare, extract, generate, initialize, filter, map, aggregate, export, import, normalize, calculate .</p>"},{"location":"coding_guidelines/#install-package","title":"Install package","text":"<p>We can install the package we are developing by the following command:</p> <pre><code>pip install -e .\n</code></pre> <p>It means we are installing it in editable mode. In Python, if you want to be able to edit your package and have the changes be reflected immediately without needing to reinstall the package every time, you can use pip to install the package in \"editable\" mode.</p> <p>If you are worried about the state of your package affecting other parts of your system or other projects, you might consider using a virtual environment. A virtual environment is an isolated Python environment, separate from your system Python and other virtual environments. You can install your package in a virtual environment and make changes and test without worrying about affecting other projects.</p>"},{"location":"coding_guidelines/#reference","title":"Reference","text":"<p>[1] Naming things properly. </p> <p>[2] Python functions naming: an algorithm.</p>"},{"location":"explanation/","title":"Explanation","text":""},{"location":"generate_docs/","title":"How to generate docs automatically","text":"<p>In this document, I will introduce how to automatically generate the documentation for your python project with several tools.</p>"},{"location":"generate_docs/#install-libraries","title":"Install libraries","text":"<p>We use the following python packages:</p> <ul> <li>MkDocs for building static pages from Markdown</li> <li>mkdocstrings for auto-generating documentation from docstrings in your code</li> <li>Material for MkDocs for styling your documentation</li> </ul> <pre><code>pip install --upgrade pip\npip install mkdocs\npip install mkdocstrings\npip install mkdocs-material\n</code></pre> <p>You can install support for specific languages using extras, for example:</p> <pre><code>pip install 'mkdocstrings[crystal,python]'\n</code></pre> <p>Note: the support for specific languages are not installed by default, so I would recommend install by the above command.</p>"},{"location":"generate_docs/#create-mkdocs-project","title":"Create mkdocs project","text":"<p>Now assume you are in the root directory of your project:</p> <pre><code>mkdocs new .\n</code></pre> <p>You will see:</p> <pre><code>INFO    -  Writing config file: ./mkdocs.yml\nINFO    -  Writing initial docs: ./docs/index.md\n</code></pre> <p>MkDocs comes with a built-in dev-server that lets you preview your documentation as you work on it. Make sure you're in the same directory as the <code>mkdocs.yml</code> configuration file, and then start the server by running the <code>mkdocs serve</code> command:</p> <pre><code>% mkdocs serve\nINFO    -  Building documentation...\nINFO    -  Cleaning site directory\nWARNING -  Excluding 'README.md' from the site because it conflicts with\n           'index.md'.\nINFO    -  Documentation built in 0.08 seconds\nINFO    -  [14:25:59] Watching paths for changes: 'docs', 'mkdocs.yml'\nINFO    -  [14:25:59] Serving on http://127.0.0.1:8000/\nINFO    -  [14:26:11] Browser connected: http://127.0.0.1:8000/\n</code></pre> <p>Open up http://127.0.0.1:8000/ in your browser, and you'll see the default home page being displayed.</p>"},{"location":"generate_docs/#customize-your-mkdocsyml","title":"Customize your mkdocs.yml","text":"<p>We can customize the style of our documentation. Edit the ./mkdocs.yml file:</p> <pre><code>site_name: your-project-name\nsite_url: your-project-website\nnav:\n  - Home: index.md\ntheme:\n  name: \"material\"\n</code></pre> <p>This way, we can use the material theme. You can also use other themes [1,2].</p>"},{"location":"generate_docs/#add-more-markdown-files-to-the-documentation","title":"Add more markdown files to the documentation","text":"<p>As described in [1], we can follow the structure proposed in the Di\u00e1taxis documentation framework, which suggests splitting your documentation into four distinct parts:</p> <ul> <li>Tutorials</li> <li>How-To Guides</li> <li>Reference</li> <li>Explanation</li> </ul> <p>Therefore, we can create these markdown files and put them into the ./docs/ folder. Then we edit our mkdocs.yml configuration file to add them:</p> <pre><code>site_name: your-project-name\nsite_url: your-project-website\n\nnav:\n  - index.md\n  - tutorials.md\n  - how-to-guides.md\n  - reference.md\n  - explanation.md\n\ntheme:\n  name: \"material\"\n</code></pre> <p>We can also edit the titles for each page, adjust their order, and so on. See [1] for more details.</p>"},{"location":"generate_docs/#generate-document-from-docstrings","title":"Generate document from Docstrings","text":"<p>We need to use <code>mkdocstrings</code> package for this purpose.</p> <p>MkDocs is a static-site generator geared toward writing documentation. However, you can\u2019t fetch docstring information from your code using MkDocs alone. You can make it work with an additional package called mkdocstrings.</p> <p>You already installed mkdocstrings into your virtual environment at the beginning of this tutorial, so you only need to add it as a plugin to your MkDocs configuration file:</p> <pre><code>site_name: your-project-name\nsite_url: your-project-website\n\nplugins:\n  - mkdocstrings\n\nnav:\n  - index.md\n  - tutorials.md\n  - how-to-guides.md\n  - reference.md\n  - explanation.md\n\ntheme:\n  name: \"material\"\n</code></pre> <p>Now, to generate documentation from soruce code docstrings, we can select a markdown file, e.g., the reference.md file we have created, and put identifiers in it.</p> <p>Mkdocstrings allows you to insert docstring information right into your Markdown pages using a special syntax of three colons (:::) followed by the code identifier that you want to document:</p> <pre><code>::: identifier\n</code></pre> <p>The identifier is a string identifying the object you want to document. The format of an <code>identifier</code> can vary from one handler to another. For example, the Python handler expects the full dotted-path to a Python object: <code>my_package.my_module.MyClass.my_method</code> [3]. </p> <p>The syntax to use identifier is:</p> <pre><code>::: identifier\n    YAML block\n</code></pre> <p>See https://mkdocstrings.github.io/usage/ for more details.</p> <p>Basically, the YAML block is optional, and contains some configuration options.</p> <p>For global options, we can put it in <code>mkdocs.yml</code>. For example:</p> <pre><code>plugins:\n- mkdocstrings:\n    enabled: !ENV [ENABLE_MKDOCSTRINGS, true]\n    custom_templates: templates\n    default_handler: python\n    handlers:\n      python:\n        options:\n          show_source: false\n</code></pre> <p>And global configurations can be overridden by local configurations.</p> <p>See [3] for more detailed tutorials. Briefly summarize, with mkdocstrings, we can use identifiers to gather the docstrings in our code and turn them into documentation.</p> <p>Tips: Maintain a good coding style is very important. I prefer to use the docstring style listed here: https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings</p>"},{"location":"generate_docs/#automatically-collect-all-the-docstrings-in-a-module","title":"Automatically collect all the docstrings in a module","text":"<p>To avoid manually write the identifiers for each submodule/class/method in a markdown file to include the corresponding docstrings in our documentation, we can use the following option:</p> <pre><code>::: src.aeiva.agent\n    options:\n      show_submodules: true\n</code></pre> <p>The above example will automatically introduce all the docstrings in the aeiva.agent package into our documentation. </p>"},{"location":"generate_docs/#advanced-theme-customization","title":"Advanced Theme Customization","text":""},{"location":"generate_docs/#changing-the-logo-and-icons","title":"Changing the logo and icons","text":"<p>See: https://squidfunk.github.io/mkdocs-material/setup/changing-the-logo-and-icons/</p>"},{"location":"generate_docs/#customize-the-landing-home-page","title":"Customize the landing home page","text":"<p>We can further customize the home page of our documentation.</p> <p>First, set your custom_dir in mkdocs.yml:</p> <pre><code>theme:\n  custom_dir: docs/overrides\n...\n\n</code></pre> <p>The above setting use overrides directory in docs/ as the custom directory.</p> <p>We than copy all the contents in: https://github.com/squidfunk/mkdocs-material/tree/master/src/.overrides to our <code>docs/overrides/</code> folder.</p> <p>Next, in the front matter of your index.md, you need to specify the template to use (copy below to index.md):</p> <pre><code>---\ntitle: Title\ntemplate: home.html\n---\n</code></pre> <p>One important thing that took me a while to realize: you need a newline at the end of your md file. If you don't have one, the content will not display [6]. </p> <p>Finally, we can customize the <code>home.html</code> and <code>main.html</code> in the overrides folder to make it consistent with our project.</p> <p>See [6] for a reference.</p> <p>Note: I found the landing page on https://squidfunk.github.io/mkdocs-material/ is really fancy! It is based on Parallax Image Effect using html and css. To DIY the effect, I downloaded the source file of the webpage directly, and then replace all <code>assets/images/layers/</code> in the html source file with <code>./Material for MkDocs_files/</code>. Because this is the only folder I can get with downloading. I haven't done with understanding and customizing the landing homepage based on this template. To be tested in the future. :) (I put this verion in docs/overrides-dev/)</p>"},{"location":"generate_docs/#organize-your-documentation","title":"Organize your documentation","text":""},{"location":"generate_docs/#navbar-nesting","title":"Navbar nesting","text":"<p>You can add an additional level to your navbar like this:</p> <pre><code>nav:\n  - Home: index.md\n  - About: about.md\n  - Foo:\n      - Overview: foo/index.md\n      - Bar: foo/bar.md\n</code></pre>"},{"location":"generate_docs/#reference-to-another-markdown-file","title":"Reference to another markdown file","text":"<p>In a markdown document, we can refer to another file from one file, like the following:</p> <pre><code>[How to generate project documentation automatically from docstrings](./GENERATE_DOCS.md)\n</code></pre>"},{"location":"generate_docs/#deploy-your-documentation-to-github","title":"Deploy Your Documentation to GitHub","text":"<p>GitHub repositories automatically serve static content when committed to a branch named gh-pages. MkDocs integrates with that and allows you to build and deploy your project documentation in a single step:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Running this command rebuilds the documentation from your Markdown files and source code and pushes it to the gh-pages branch on your remote GitHub repository.</p> <p>Because of GitHub\u2019s default configuration, that\u2019ll make your documentation available at the URL that MkDocs shows you at the end of your terminal output:</p> <pre><code>INFO - Your documentation should shortly be available at:\n       https://user-name.github.io/project-name/\n</code></pre>"},{"location":"generate_docs/#summarize","title":"Summarize","text":"<p>So we basically follow the following procedures to create our documentation:</p> <ol> <li>Create virtual env for your project. Create your project. Create your github repository.</li> <li>Install the libraries: mkdocs, mkdocstrings, mkdocs-material</li> <li>Go to the project root directory.</li> <li>Use mkdocs to create the docs. It will produce <code>mkdocs.yml</code> and <code>./docs/index.md</code>.</li> <li>Customize the <code>mkdocs.yml</code>. Basically, this is the global setting of the documentation. See [2] for details. You can customize your documentation theme to <code>materials</code> theme that supported by <code>mkdocs-material</code> python package.</li> <li>Customize the contents in <code>./docs/</code>. Basically, you can create different markdown files here; you can automatically create documentation contents from docstrings of your code by using <code>::: identifier</code> that supported by <code>mkdocstrings</code>. See [4] for details.</li> <li>Customize the organization of your documentation. For example, you can use nested navigation; you can use cross-reference, etc.</li> <li>Build your documentation using ```mkdocs build.</li> <li>Host your documentation using <code>mkdocs gh-deploy</code>. Your documentation should shortly be available at: <code>https://user-name.github.io/project-name/</code>.</li> </ol>"},{"location":"generate_docs/#more","title":"More","text":"<p>Please read [1,2,3,4] for more detailed tutorials.</p>"},{"location":"generate_docs/#reference","title":"Reference","text":"<p>[1] Build Your Python Project Documentation With MkDocs</p> <p>[2] Getting Started with MkDocs</p> <p>[3] mkdocstrings.github.io/</p> <p>[4] mkdocs-material/</p> <p>[5] Di\u00e1taxis A systematic approach to technical documentation authoring.</p> <p>[6] https://github.com/squidfunk/mkdocs-material/issues/1996</p>"},{"location":"how-to-guides/","title":"How-to-guides","text":"<p>Here we summarize some experience we learned during developing Aeiva.</p> <p>How to generate project documentation automatically from docstrings</p>"},{"location":"install_minedojo/","title":"Install MineDojo platform on MacBook Pro with M1 Chip","text":""},{"location":"install_minedojo/#setup-java-environment","title":"Setup Java Environment","text":"<p>I followed the instructions on: https://docs.minedojo.org/sections/getting_started/install.html#prerequisites</p> <p>Specifically, remember to list all installed Java and and export the temurin8 version java:</p> <pre><code>/usr/libexec/java_home -V\nexport JAVA_HOME=path/to/eclipse/temurin8\n</code></pre> <p>After run</p> <pre><code>java -version\n</code></pre> <p>I got</p> <pre><code>openjdk version \"1.8.0_332\"\nOpenJDK Runtime Environment (Temurin)(build 1.8.0_332-b09)\nOpenJDK 64-Bit Server VM (Temurin)(build 25.332-b09, mixed mode)\n</code></pre>"},{"location":"install_minedojo/#install-minedojo","title":"Install MineDojo","text":"<p>I used the following command: (Assume Java JDK 8 is already installed)</p> <pre><code>pip3 install setuptools==65.5.0 pip==21\npip3 install gym==0.21\ngit clone https://github.com/MineDojo/MineDojo &amp;&amp; cd MineDojo\npip install -e .\n</code></pre> <p>Note: I found that at the end, if I install from source, I cannot remove the source directory. So after resolved all the bugs as follows, I reinstalled minedojo via pip in my conda virtual env:</p> <pre><code>pip install minedojo\n</code></pre> <p>So I would recommend install via pip rather than from source.</p>"},{"location":"install_minedojo/#debug-experience","title":"Debug experience","text":"<p>There are many different bugs when I try to run</p> <pre><code>python scripts/validate_install.py\n</code></pre> <p>Below, I list all the operations I have done.</p>"},{"location":"install_minedojo/#upgraded-gradle","title":"Upgraded gradle","text":"<p>Check the following: https://gradle.org/install/</p> <p>After installed the new gradle, I got:</p> <pre><code>&gt;&gt;&gt; gradle -v\n\n------------------------------------------------------------\nGradle 8.2.1\n------------------------------------------------------------\n\nBuild time:   2023-07-10 12:12:35 UTC\nRevision:     a38ec64d3c4612da9083cc506a1ccb212afeecaa\n\nKotlin:       1.8.20\nGroovy:       3.0.17\nAnt:          Apache Ant(TM) version 1.10.13 compiled on January 4 2023\nJVM:          1.8.0_332 (Temurin 25.332-b09)\nOS:           Mac OS X 10.16 x86_64\n\n</code></pre>"},{"location":"install_minedojo/#malmo-errors","title":"Malmo errors","text":"<p>I referred to: https://github.com/MineDojo/MineDojo/issues/32#issuecomment-1237247417  It says:</p> <p>For Deprecated Gradle feature --&gt; Go to Malmo project download latest prebuild version https://github.com/Microsoft/malmo/releases. Then find and replace the Malmo directory in your python package directory @ xxx/minedojo/sim/Malmo on your computer. (Reminder directory shall keep the same name \"Malmo\")</p> <p>For \"OpenGL: ERROR RuntimeException: No OpenGL context found in the current thread.\" (X Error &amp; bad value) --&gt; make sure you run sudo apt update &amp;&amp; sudo apt upgrade before you compile the minecraft java program as the same problem has been described in https://stackoverflow.com/questions/28867285/lwjgl-reports-that-opengl-is-not-supported-on-a-modern-nvidia-card. This works for me.</p> <p>Before running python Minedojo code, go xxx/minedojo/sim/Malmo/Minecraft/ where your python put minedojo package and execute ./launchClient.sh (for linux/unix) or .\\launchClient (for windows, there's a launchClient.bat file) and make sure it can run normally before you start with Minedojo.</p> <p>Specifically, when I try to run ./launchClient.sh, I got error due to tools.jar, so I did the following:</p> <pre><code>copy tools.jar from \n/Library/Java/JavaVirtualMachines/temurin-8.jdk/Contents/Home/lib\nto\n/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/lib\n\n&gt;&gt;&gt; sudo copy XXX XXX\npasswd: (For me, it is the same as the passwd when I login to my macbook pro: the name :)\n</code></pre> <p>Then, it still fail. So I used back the original Malmo in MineDojo installation (i.e., maybe we DON'T need to download latest prebuild version https://github.com/Microsoft/malmo/releases and then find and replace the Malmo directory in your python package directory ). </p> <p>Now it can run. But still some error due to </p> <pre><code>raise NoSuchProcess(self.pid, self._name)\npsutil.NoSuchProcess: process no longer exists (pid=50957, name='bash')\n</code></pre> <p>I removed the </p> <pre><code>env.close()\n</code></pre> <p>in the script and it works.</p> <p>This is not the end of the story: I found the script doesn't always work. Sometimes, I don't need to remvoe the <code>env.close()</code> and it still works. Sometimes it doesn't work due to errors like</p> <pre><code>...\n    at org.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:163)\n    at org.apache.http.impl.conn.CPoolProxy.receiveResponseHeader(CPoolProxy.java:165)\n    at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:273)\n    at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125)\n    at org.apache.http.impl.execchain.MainClientExec.createTunnelToTarget(MainClientExec.java:473)\n    at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:398)\n    at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:237)\n    at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:185)\n    at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89)\n    at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:111)\n    at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n    at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n    at org.gradle.internal.resource.transport.http.HttpClientHelper.performHttpRequest(HttpClientHelper.java:148)\n    at org.gradle.internal.resource.transport.http.HttpClientHelper.performHttpRequest(HttpClientHelper.java:126)\n    at org.gradle.internal.resource.transport.http.HttpClientHelper.executeGetOrHead(HttpClientHelper.java:103)\n    at org.gradle.internal.resource.transport.http.HttpClientHelper.performRequest(HttpClientHelper.java:94)\n    ... 171 more\n\n\n* Get more help at https://help.gradle.org\n\nBUILD FAILED in 31s\n\n\nMinecraft process finished unexpectedly. There was an error with Malmo.\n</code></pre> <p>I suppose it is due to some network connection errors?</p> <p>Anyway, now it can work.</p>"},{"location":"intro/","title":"Introduction","text":"<p>Welcome to Aeiva! Here we will briefly introduce this project.</p>"},{"location":"reference/","title":"Reference","text":"<p>This part of the project documentation focuses on an information-oriented approach. Use it as a reference for the technical implementation of the <code>Aeiva</code> project code.</p>"},{"location":"reference/#aeiva-api-references","title":"Aeiva API references","text":""},{"location":"reference/#src.aeiva.agent","title":"<code>agent</code>","text":""},{"location":"reference/#src.aeiva.agent.action","title":"<code>action</code>","text":""},{"location":"reference/#src.aeiva.agent.action.Action","title":"<code>Action</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for actions. This represents a behavior or operation carried out by the agent, based on its decision.</p> Source code in <code>src/aeiva/agent/action.py</code> <pre><code>class Action(ABC):\n\"\"\"\n    Abstract base class for actions. This represents a behavior or operation carried out by the agent, based on its decision.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        self.actions = self.perform_action(*args, **kwargs)\n\n    @property\n    def actions(self) -&gt; List[str]:\n        return self._actions\n\n    @actions.setter\n    def actions(self, value: List[str]):\n        self._actions = value\n\n    @abstractmethod\n    def perform_action(self, *args, **kwargs) -&gt; List[str]:\n\"\"\"\n        Abstract method for performing an action based on the agent's decision.\n        Should return a list of actions to be performed by the agent.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.action.Action.perform_action","title":"<code>perform_action(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for performing an action based on the agent's decision. Should return a list of actions to be performed by the agent.</p> Source code in <code>src/aeiva/agent/action.py</code> <pre><code>@abstractmethod\ndef perform_action(self, *args, **kwargs) -&gt; List[str]:\n\"\"\"\n    Abstract method for performing an action based on the agent's decision.\n    Should return a list of actions to be performed by the agent.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.action_system","title":"<code>action_system</code>","text":""},{"location":"reference/#src.aeiva.agent.action_system.ActionSystem","title":"<code>ActionSystem</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for action systems.</p> Source code in <code>src/aeiva/agent/action_system.py</code> <pre><code>class ActionSystem(ABC):\n\"\"\"\n    Abstract base class for action systems.\n    \"\"\"\n    def __init__(self, motor_capacity: Dict[str, Union[int, float]], *args, **kwargs):\n        self.motor_capacity = motor_capacity  # The capacity of each motor function of the agent\n\n    @abstractmethod\n    def act(self, decision: 'Decision', *args, **kwargs) -&gt; 'Action':\n\"\"\"\n        Abstract method for generating an action based on the given decision.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def feedback(self, *args, **kwargs) -&gt; None:\n\"\"\"\n        Abstract method for feedback mechanisms that influence the PerceptionSystem or CognitiveSystem.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.action_system.ActionSystem.act","title":"<code>act(decision, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for generating an action based on the given decision.</p> Source code in <code>src/aeiva/agent/action_system.py</code> <pre><code>@abstractmethod\ndef act(self, decision: 'Decision', *args, **kwargs) -&gt; 'Action':\n\"\"\"\n    Abstract method for generating an action based on the given decision.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.action_system.ActionSystem.feedback","title":"<code>feedback(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for feedback mechanisms that influence the PerceptionSystem or CognitiveSystem.</p> Source code in <code>src/aeiva/agent/action_system.py</code> <pre><code>@abstractmethod\ndef feedback(self, *args, **kwargs) -&gt; None:\n\"\"\"\n    Abstract method for feedback mechanisms that influence the PerceptionSystem or CognitiveSystem.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.agent","title":"<code>agent</code>","text":""},{"location":"reference/#src.aeiva.agent.agent.AgentBase","title":"<code>AgentBase</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>src/aeiva/agent/agent.py</code> <pre><code>class AgentBase(ABC):\n    def __init__(self, \n                 id: str, \n                 name: str, \n                 perception_system: PerceptionSystem, \n                 cognitive_system: CognitiveSystem, \n                 action_system: ActionSystem,\n                 role: Optional[Role] = None,\n                 background: Optional[Background] = None,\n                 relationships: Optional[Dict[str, Relationship]] = None,\n                 *args,\n                 **kwargs):\n        self.id = id\n        self.name = name\n        self.perception_system = perception_system\n        self.cognitive_system = cognitive_system\n        self.action_system = action_system\n        self.memory = {}\n        self.world_model = None\n        self.goal = None\n        self.role = role\n        self.background = background\n        self.relationships = relationships if relationships else {}\n\n    @abstractmethod\n    def perceive(self, stimuli: Stimuli, *args, **kwargs):\n\"\"\"\n        The agent takes in stimuli from the environment and processes it,\n        the result is then passed to the cognitive system for further processing.\n        \"\"\"\n        processed_data = self.perception_system.process(stimuli)\n        self.memory['last_perception'] = processed_data\n        return processed_data\n\n    @abstractmethod\n    def think(self, *args, **kwargs):\n\"\"\"\n        The agent uses its cognitive system to plan actions based on the current state\n        of the world and its goals. It also uses its memory and potentially updates its world model.\n        \"\"\"\n        cognitive_output = self.cognitive_system.process(self.memory)\n        self.memory['last_cognition'] = cognitive_output\n        return cognitive_output\n\n    @abstractmethod\n    def act(self, *args, **kwargs):\n\"\"\"\n        The agent performs an action in the environment using its action system,\n        this action is based on the output from the cognitive system.\n        \"\"\"\n        action = self.cognitive_system.get_last_action()\n        self.action_system.perform(action)\n\n    @abstractmethod\n    def learn(self, *args, **kwargs):\n\"\"\"\n        The agent updates its perception, cognition, and action systems based on the feedback it gets from the environment,\n        this could be done using various machine learning techniques.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def set_goal(self, goal: Any, *args, **kwargs):\n\"\"\"\n        The agent sets its goal, which could guide its cognitive system in making decisions.\n        \"\"\"\n        self.goal = goal\n\n    @abstractmethod\n    def update_world_model(self, world_model: WorldModel, *args, **kwargs):\n\"\"\"\n        The agent updates its world model, which could be used in the cognitive system for making decisions.\n        \"\"\"\n        self.world_model = world_model\n\n    @abstractmethod\n    def set_role(self, role: Role):\n\"\"\"\n        The agent sets its role in the society, which could guide its actions and interactions with other agents.\n        \"\"\"\n        self.role = role\n\n    @abstractmethod\n    def set_background(self, background: Background):\n\"\"\"\n        The agent sets its background, which could influence its behavior and interactions with other agents.\n        \"\"\"\n        self.background = background\n\n    @abstractmethod\n    def add_relationship(self, other_agent_id: str, relationship: Relationship):\n\"\"\"\n        The agent adds a relationship with another agent, which could influence its behavior and interactions with the other agent.\n        \"\"\"\n        self.relationships[other_agent_id] = relationship\n\n    @abstractmethod\n    def remove_relationship(self, other_agent_id: str):\n\"\"\"\n        The agent removes a relationship with another agent.\n        \"\"\"\n        if other_agent_id in self.relationships:\n            del self.relationships[other_agent_id]\n</code></pre>"},{"location":"reference/#src.aeiva.agent.agent.AgentBase.act","title":"<code>act(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>The agent performs an action in the environment using its action system, this action is based on the output from the cognitive system.</p> Source code in <code>src/aeiva/agent/agent.py</code> <pre><code>@abstractmethod\ndef act(self, *args, **kwargs):\n\"\"\"\n    The agent performs an action in the environment using its action system,\n    this action is based on the output from the cognitive system.\n    \"\"\"\n    action = self.cognitive_system.get_last_action()\n    self.action_system.perform(action)\n</code></pre>"},{"location":"reference/#src.aeiva.agent.agent.AgentBase.add_relationship","title":"<code>add_relationship(other_agent_id, relationship)</code>  <code>abstractmethod</code>","text":"<p>The agent adds a relationship with another agent, which could influence its behavior and interactions with the other agent.</p> Source code in <code>src/aeiva/agent/agent.py</code> <pre><code>@abstractmethod\ndef add_relationship(self, other_agent_id: str, relationship: Relationship):\n\"\"\"\n    The agent adds a relationship with another agent, which could influence its behavior and interactions with the other agent.\n    \"\"\"\n    self.relationships[other_agent_id] = relationship\n</code></pre>"},{"location":"reference/#src.aeiva.agent.agent.AgentBase.learn","title":"<code>learn(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>The agent updates its perception, cognition, and action systems based on the feedback it gets from the environment, this could be done using various machine learning techniques.</p> Source code in <code>src/aeiva/agent/agent.py</code> <pre><code>@abstractmethod\ndef learn(self, *args, **kwargs):\n\"\"\"\n    The agent updates its perception, cognition, and action systems based on the feedback it gets from the environment,\n    this could be done using various machine learning techniques.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.agent.AgentBase.perceive","title":"<code>perceive(stimuli, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>The agent takes in stimuli from the environment and processes it, the result is then passed to the cognitive system for further processing.</p> Source code in <code>src/aeiva/agent/agent.py</code> <pre><code>@abstractmethod\ndef perceive(self, stimuli: Stimuli, *args, **kwargs):\n\"\"\"\n    The agent takes in stimuli from the environment and processes it,\n    the result is then passed to the cognitive system for further processing.\n    \"\"\"\n    processed_data = self.perception_system.process(stimuli)\n    self.memory['last_perception'] = processed_data\n    return processed_data\n</code></pre>"},{"location":"reference/#src.aeiva.agent.agent.AgentBase.remove_relationship","title":"<code>remove_relationship(other_agent_id)</code>  <code>abstractmethod</code>","text":"<p>The agent removes a relationship with another agent.</p> Source code in <code>src/aeiva/agent/agent.py</code> <pre><code>@abstractmethod\ndef remove_relationship(self, other_agent_id: str):\n\"\"\"\n    The agent removes a relationship with another agent.\n    \"\"\"\n    if other_agent_id in self.relationships:\n        del self.relationships[other_agent_id]\n</code></pre>"},{"location":"reference/#src.aeiva.agent.agent.AgentBase.set_background","title":"<code>set_background(background)</code>  <code>abstractmethod</code>","text":"<p>The agent sets its background, which could influence its behavior and interactions with other agents.</p> Source code in <code>src/aeiva/agent/agent.py</code> <pre><code>@abstractmethod\ndef set_background(self, background: Background):\n\"\"\"\n    The agent sets its background, which could influence its behavior and interactions with other agents.\n    \"\"\"\n    self.background = background\n</code></pre>"},{"location":"reference/#src.aeiva.agent.agent.AgentBase.set_goal","title":"<code>set_goal(goal, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>The agent sets its goal, which could guide its cognitive system in making decisions.</p> Source code in <code>src/aeiva/agent/agent.py</code> <pre><code>@abstractmethod\ndef set_goal(self, goal: Any, *args, **kwargs):\n\"\"\"\n    The agent sets its goal, which could guide its cognitive system in making decisions.\n    \"\"\"\n    self.goal = goal\n</code></pre>"},{"location":"reference/#src.aeiva.agent.agent.AgentBase.set_role","title":"<code>set_role(role)</code>  <code>abstractmethod</code>","text":"<p>The agent sets its role in the society, which could guide its actions and interactions with other agents.</p> Source code in <code>src/aeiva/agent/agent.py</code> <pre><code>@abstractmethod\ndef set_role(self, role: Role):\n\"\"\"\n    The agent sets its role in the society, which could guide its actions and interactions with other agents.\n    \"\"\"\n    self.role = role\n</code></pre>"},{"location":"reference/#src.aeiva.agent.agent.AgentBase.think","title":"<code>think(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>The agent uses its cognitive system to plan actions based on the current state of the world and its goals. It also uses its memory and potentially updates its world model.</p> Source code in <code>src/aeiva/agent/agent.py</code> <pre><code>@abstractmethod\ndef think(self, *args, **kwargs):\n\"\"\"\n    The agent uses its cognitive system to plan actions based on the current state\n    of the world and its goals. It also uses its memory and potentially updates its world model.\n    \"\"\"\n    cognitive_output = self.cognitive_system.process(self.memory)\n    self.memory['last_cognition'] = cognitive_output\n    return cognitive_output\n</code></pre>"},{"location":"reference/#src.aeiva.agent.agent.AgentBase.update_world_model","title":"<code>update_world_model(world_model, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>The agent updates its world model, which could be used in the cognitive system for making decisions.</p> Source code in <code>src/aeiva/agent/agent.py</code> <pre><code>@abstractmethod\ndef update_world_model(self, world_model: WorldModel, *args, **kwargs):\n\"\"\"\n    The agent updates its world model, which could be used in the cognitive system for making decisions.\n    \"\"\"\n    self.world_model = world_model\n</code></pre>"},{"location":"reference/#src.aeiva.agent.background","title":"<code>background</code>","text":""},{"location":"reference/#src.aeiva.agent.background.Background","title":"<code>Background</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for the background of an agent in a society.</p> Source code in <code>src/aeiva/agent/background.py</code> <pre><code>class Background(ABC):\n\"\"\"\n    Abstract base class for the background of an agent in a society.\n    \"\"\"\n    def __init__(self, background_info: Dict[str, Any], *args, **kwargs):\n        self._background_info = background_info\n\n    @property\n    def background_info(self) -&gt; Dict[str, Any]:\n        return self._background_info\n\n    def add_background_info(self, key: str, value: Any):\n        self._background_info[key] = value\n\n    @abstractmethod\n    def update_background(self, *args, **kwargs):\n        pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.cognitive_system","title":"<code>cognitive_system</code>","text":""},{"location":"reference/#src.aeiva.agent.cognitive_system.CognitiveSystem","title":"<code>CognitiveSystem</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for cognitive systems.</p> Source code in <code>src/aeiva/agent/cognitive_system.py</code> <pre><code>class CognitiveSystem(ABC):\n\"\"\"\n    Abstract base class for cognitive systems.\n    \"\"\"\n    def __init__(self, memory: 'Memory', world_model: 'WorldModel', *args, **kwargs):\n        self.memory = memory  # The memory system of the agent\n        self.world_model = world_model  # The world model of the agent\n\n    @abstractmethod\n    def think(self, observation: 'Observation', *args, **kwargs) -&gt; 'Thought':\n\"\"\"\n        Abstract method for generating a thought based on the given observation.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def parallel_processing(self, *args, **kwargs) -&gt; 'Thought':\n\"\"\"\n        Abstract method for parallel processing of different types of cognitive processes.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.cognitive_system.CognitiveSystem.parallel_processing","title":"<code>parallel_processing(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for parallel processing of different types of cognitive processes.</p> Source code in <code>src/aeiva/agent/cognitive_system.py</code> <pre><code>@abstractmethod\ndef parallel_processing(self, *args, **kwargs) -&gt; 'Thought':\n\"\"\"\n    Abstract method for parallel processing of different types of cognitive processes.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.cognitive_system.CognitiveSystem.think","title":"<code>think(observation, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for generating a thought based on the given observation.</p> Source code in <code>src/aeiva/agent/cognitive_system.py</code> <pre><code>@abstractmethod\ndef think(self, observation: 'Observation', *args, **kwargs) -&gt; 'Thought':\n\"\"\"\n    Abstract method for generating a thought based on the given observation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.decision","title":"<code>decision</code>","text":""},{"location":"reference/#src.aeiva.agent.decision.Decision","title":"<code>Decision</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for decisions. This represents a choice made by the agent, based on its thoughts.</p> Source code in <code>src/aeiva/agent/decision.py</code> <pre><code>class Decision(ABC):\n\"\"\"\n    Abstract base class for decisions. This represents a choice made by the agent, based on its thoughts.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        self.action_plan = self.make_decision(*args, **kwargs)\n\n    @property\n    def action_plan(self) -&gt; Dict[str, Any]:\n        return self._action_plan\n\n    @action_plan.setter\n    def action_plan(self, value: Dict[str, Any]):\n        self._action_plan = value\n\n    @abstractmethod\n    def make_decision(self, *args, **kwargs) -&gt; Dict[str, Any]:\n\"\"\"\n        Abstract method for making a decision based on the agent's thoughts.\n        Should return a plan of action the agent has decided on.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.decision.Decision.make_decision","title":"<code>make_decision(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for making a decision based on the agent's thoughts. Should return a plan of action the agent has decided on.</p> Source code in <code>src/aeiva/agent/decision.py</code> <pre><code>@abstractmethod\ndef make_decision(self, *args, **kwargs) -&gt; Dict[str, Any]:\n\"\"\"\n    Abstract method for making a decision based on the agent's thoughts.\n    Should return a plan of action the agent has decided on.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.env","title":"<code>env</code>","text":""},{"location":"reference/#src.aeiva.agent.env.Environment","title":"<code>Environment</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for an environment in which agents interact. This follows a similar interface as OpenAI Gym environments.</p> Source code in <code>src/aeiva/agent/env.py</code> <pre><code>class Environment(ABC):\n\"\"\"\n    Abstract base class for an environment in which agents interact. This follows a similar interface as OpenAI Gym environments.\n    \"\"\"\n\n    def __init__(self, name: str, description: Optional[str] = None, *args, **kwargs):\n        self._name = name\n        self._description = description\n        self._societies = []  # List to store societies present in the environment\n        self._resources = {}  # Resources available in the environment\n\n        self._action_space = None  # Specifies the valid actions an agent can take in the environment\n        self._observation_space = None  # Specifies the structure of observations an agent can receive from the environment\n\n    @property\n    def name(self) -&gt; str:\n        return self._name\n\n    @property\n    def description(self) -&gt; Optional[str]:\n        return self._description\n\n    @abstractmethod\n    def step(self, action: Any) -&gt; Tuple[Any, float, bool, Dict]:\n\"\"\"\n        Execute one time step within the environment. This should return four values:\n        - observation (object): agent's observation of the current environment\n        - reward (float) : amount of reward returned after previous action\n        - done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n        - info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def reset(self) -&gt; Any:\n\"\"\"\n        Resets the state of the environment and returns an initial observation.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def render(self, mode: str = 'human'):\n\"\"\"\n        Renders the environment. The set of supported modes varies per environment.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def close(self):\n\"\"\"\n        Clean up the environment's resources.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def add_society(self, society: Society):\n\"\"\"\n        Abstract method for adding a society to the environment. This should be implemented in a concrete subclass \n        according to the specific behavior of the environment.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove_society(self, society: Society):\n\"\"\"\n        Abstract method for removing a society from the environment. This should be implemented in a concrete \n        subclass according to the specific behavior of the environment.\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def action_space(self) -&gt; 'Space':\n\"\"\"\n        Returns the Space object corresponding to valid actions.\n        \"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def observation_space(self) -&gt; 'Space':\n\"\"\"\n        Returns the Space object corresponding to valid observations.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.env.Environment.action_space","title":"<code>action_space: Space</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the Space object corresponding to valid actions.</p>"},{"location":"reference/#src.aeiva.agent.env.Environment.observation_space","title":"<code>observation_space: Space</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the Space object corresponding to valid observations.</p>"},{"location":"reference/#src.aeiva.agent.env.Environment.add_society","title":"<code>add_society(society)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for adding a society to the environment. This should be implemented in a concrete subclass  according to the specific behavior of the environment.</p> Source code in <code>src/aeiva/agent/env.py</code> <pre><code>@abstractmethod\ndef add_society(self, society: Society):\n\"\"\"\n    Abstract method for adding a society to the environment. This should be implemented in a concrete subclass \n    according to the specific behavior of the environment.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.env.Environment.close","title":"<code>close()</code>  <code>abstractmethod</code>","text":"<p>Clean up the environment's resources.</p> Source code in <code>src/aeiva/agent/env.py</code> <pre><code>@abstractmethod\ndef close(self):\n\"\"\"\n    Clean up the environment's resources.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.env.Environment.remove_society","title":"<code>remove_society(society)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for removing a society from the environment. This should be implemented in a concrete  subclass according to the specific behavior of the environment.</p> Source code in <code>src/aeiva/agent/env.py</code> <pre><code>@abstractmethod\ndef remove_society(self, society: Society):\n\"\"\"\n    Abstract method for removing a society from the environment. This should be implemented in a concrete \n    subclass according to the specific behavior of the environment.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.env.Environment.render","title":"<code>render(mode='human')</code>  <code>abstractmethod</code>","text":"<p>Renders the environment. The set of supported modes varies per environment.</p> Source code in <code>src/aeiva/agent/env.py</code> <pre><code>@abstractmethod\ndef render(self, mode: str = 'human'):\n\"\"\"\n    Renders the environment. The set of supported modes varies per environment.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.env.Environment.reset","title":"<code>reset()</code>  <code>abstractmethod</code>","text":"<p>Resets the state of the environment and returns an initial observation.</p> Source code in <code>src/aeiva/agent/env.py</code> <pre><code>@abstractmethod\ndef reset(self) -&gt; Any:\n\"\"\"\n    Resets the state of the environment and returns an initial observation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.env.Environment.step","title":"<code>step(action)</code>  <code>abstractmethod</code>","text":"<p>Execute one time step within the environment. This should return four values: - observation (object): agent's observation of the current environment - reward (float) : amount of reward returned after previous action - done (bool): whether the episode has ended, in which case further step() calls will return undefined results - info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)</p> Source code in <code>src/aeiva/agent/env.py</code> <pre><code>@abstractmethod\ndef step(self, action: Any) -&gt; Tuple[Any, float, bool, Dict]:\n\"\"\"\n    Execute one time step within the environment. This should return four values:\n    - observation (object): agent's observation of the current environment\n    - reward (float) : amount of reward returned after previous action\n    - done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n    - info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.memory","title":"<code>memory</code>","text":""},{"location":"reference/#src.aeiva.agent.memory.Memory","title":"<code>Memory</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for the memory of an agent. This incorporates concepts from neuroscience and machine learning.</p> Source code in <code>src/aeiva/agent/memory.py</code> <pre><code>class Memory(ABC):\n\"\"\"\n    Abstract base class for the memory of an agent. This incorporates concepts from neuroscience and machine learning.\n    \"\"\"\n    def __init__(self, short_term_capacity: int, long_term_capacity: int):\n        self.short_term_memory = {}  # Working memory, storing temporary information\n        self.long_term_memory = {}  # Long term memory, storing persisting information\n        self.short_term_capacity = short_term_capacity\n        self.long_term_capacity = long_term_capacity\n        self.access_times = {}  # Keep track of access times for each memory item\n        self.short_term_queue = deque(maxlen=short_term_capacity)  # Queue to keep track of addition order in short term memory\n        self.long_term_queue = deque(maxlen=long_term_capacity)  # Queue to keep track of addition order in long term memory\n\n    def remember(self, key: str, value: Any, term: str = \"short\"):\n        if term == \"short\":\n            self.short_term_queue.append(key)\n            if len(self.short_term_memory) &gt;= self.short_term_capacity:\n                self.forget(\"short\")\n            self.short_term_memory[key] = value\n        else:\n            self.long_term_queue.append(key)\n            if len(self.long_term_memory) &gt;= self.long_term_capacity:\n                self.forget(\"long\")\n            self.long_term_memory[key] = value\n        self.access_times[key] = 0  # Initialize access times\n\n    def retrieve(self, key: str, term: str = \"short\"):\n        if term == \"short\":\n            self.access_times[key] += 1  # Increase access times\n            return self.short_term_memory.get(key)\n        else:\n            self.access_times[key] += 1  # Increase access times\n            return self.long_term_memory.get(key)\n\n    def forget(self, term: str = \"short\"):\n        # Forget based on 'use-it-or-lose-it' principle\n        if term == \"short\":\n            oldest_item_key = self.short_term_queue.popleft()\n            self.short_term_memory.pop(oldest_item_key)\n            self.access_times.pop(oldest_item_key)\n        else:\n            oldest_item_key = self.long_term_queue.popleft()\n            self.long_term_memory.pop(oldest_item_key)\n            self.access_times.pop(oldest_item_key)\n\n    @abstractmethod\n    def decay(self):\n\"\"\"\n        Abstract method for decay function of memories over time. This should be implemented in a more specific subclass.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def reinforce(self, key: str):\n\"\"\"\n        Abstract method for reinforcement function of memories over time. This should be implemented in a more specific subclass.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def search_memory(self, query: Any) -&gt; Tuple[Optional[Dict], Optional[Dict]]:\n\"\"\"\n        Abstract method for searching both short and long term memories for the query and returns the found item. \n        This should be implemented in a more specific subclass using ML techniques.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def self_organize(self):\n\"\"\"\n        Abstract method for self-organizing memory.\n        This should be implemented in a more specific subclass.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.memory.Memory.decay","title":"<code>decay()</code>  <code>abstractmethod</code>","text":"<p>Abstract method for decay function of memories over time. This should be implemented in a more specific subclass.</p> Source code in <code>src/aeiva/agent/memory.py</code> <pre><code>@abstractmethod\ndef decay(self):\n\"\"\"\n    Abstract method for decay function of memories over time. This should be implemented in a more specific subclass.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.memory.Memory.reinforce","title":"<code>reinforce(key)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for reinforcement function of memories over time. This should be implemented in a more specific subclass.</p> Source code in <code>src/aeiva/agent/memory.py</code> <pre><code>@abstractmethod\ndef reinforce(self, key: str):\n\"\"\"\n    Abstract method for reinforcement function of memories over time. This should be implemented in a more specific subclass.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.memory.Memory.search_memory","title":"<code>search_memory(query)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for searching both short and long term memories for the query and returns the found item.  This should be implemented in a more specific subclass using ML techniques.</p> Source code in <code>src/aeiva/agent/memory.py</code> <pre><code>@abstractmethod\ndef search_memory(self, query: Any) -&gt; Tuple[Optional[Dict], Optional[Dict]]:\n\"\"\"\n    Abstract method for searching both short and long term memories for the query and returns the found item. \n    This should be implemented in a more specific subclass using ML techniques.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.memory.Memory.self_organize","title":"<code>self_organize()</code>  <code>abstractmethod</code>","text":"<p>Abstract method for self-organizing memory. This should be implemented in a more specific subclass.</p> Source code in <code>src/aeiva/agent/memory.py</code> <pre><code>@abstractmethod\ndef self_organize(self):\n\"\"\"\n    Abstract method for self-organizing memory.\n    This should be implemented in a more specific subclass.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.observation","title":"<code>observation</code>","text":""},{"location":"reference/#src.aeiva.agent.observation.Observation","title":"<code>Observation</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for observations. This represents the agent's internal representation of the stimuli. The actual processing of raw_data into processed_data is left as an abstract method, allowing subclasses  to define the specific transformation pipeline.</p> Source code in <code>src/aeiva/agent/observation.py</code> <pre><code>class Observation(ABC):\n\"\"\"\n    Abstract base class for observations. This represents the agent's internal representation of the stimuli.\n    The actual processing of raw_data into processed_data is left as an abstract method, allowing subclasses \n    to define the specific transformation pipeline.\n    \"\"\"\n    def __init__(self, raw_data: Dict[str, Any], *args, **kwargs):\n        self.raw_data = raw_data  # Raw observation data\n        self.processed_data = self.process_raw_data(self.raw_data, *args, **kwargs)  # Processed observation data\n\n    @property\n    def raw_data(self) -&gt; Dict[str, Any]:\n        return self._raw_data\n\n    @raw_data.setter\n    def raw_data(self, value: Dict[str, Any]):\n        self._raw_data = value\n\n    @property\n    def processed_data(self) -&gt; Dict[str, Any]:\n        return self._processed_data\n\n    @processed_data.setter\n    def processed_data(self, value: Dict[str, Any]):\n        self._processed_data = value\n\n    @abstractmethod\n    def process_raw_data(self, raw_data: Dict[str, Any], *args, **kwargs) -&gt; Dict[str, Any]:\n\"\"\"\n        Abstract method for processing the raw_data. The actual processing pipeline should be defined in subclasses.\n        The output is stored in the processed_data attribute.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.observation.Observation.process_raw_data","title":"<code>process_raw_data(raw_data, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for processing the raw_data. The actual processing pipeline should be defined in subclasses. The output is stored in the processed_data attribute.</p> Source code in <code>src/aeiva/agent/observation.py</code> <pre><code>@abstractmethod\ndef process_raw_data(self, raw_data: Dict[str, Any], *args, **kwargs) -&gt; Dict[str, Any]:\n\"\"\"\n    Abstract method for processing the raw_data. The actual processing pipeline should be defined in subclasses.\n    The output is stored in the processed_data attribute.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.perception_system","title":"<code>perception_system</code>","text":""},{"location":"reference/#src.aeiva.agent.perception_system.PerceptionSystem","title":"<code>PerceptionSystem</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for perception systems.</p> Source code in <code>src/aeiva/agent/perception_system.py</code> <pre><code>class PerceptionSystem(ABC):\n\"\"\"\n    Abstract base class for perception systems.\n    \"\"\"\n    def __init__(self, sensory_capacity: Dict[str, Union[int, float]], *args, **kwargs):\n        self.sensory_capacity = sensory_capacity  # The capacity of each sensory organ of the agent\n\n    @abstractmethod\n    def perceive(self, stimuli: 'Stimuli', *args, **kwargs) -&gt; 'Observation':\n\"\"\"\n        Abstract method for processing the given stimuli into an internal observation.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def hierarchical_processing(self, *args, **kwargs) -&gt; 'Observation':\n\"\"\"\n        Abstract method for hierarchical processing of stimuli.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.perception_system.PerceptionSystem.hierarchical_processing","title":"<code>hierarchical_processing(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for hierarchical processing of stimuli.</p> Source code in <code>src/aeiva/agent/perception_system.py</code> <pre><code>@abstractmethod\ndef hierarchical_processing(self, *args, **kwargs) -&gt; 'Observation':\n\"\"\"\n    Abstract method for hierarchical processing of stimuli.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.perception_system.PerceptionSystem.perceive","title":"<code>perceive(stimuli, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for processing the given stimuli into an internal observation.</p> Source code in <code>src/aeiva/agent/perception_system.py</code> <pre><code>@abstractmethod\ndef perceive(self, stimuli: 'Stimuli', *args, **kwargs) -&gt; 'Observation':\n\"\"\"\n    Abstract method for processing the given stimuli into an internal observation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.relationship","title":"<code>relationship</code>","text":""},{"location":"reference/#src.aeiva.agent.relationship.Relationship","title":"<code>Relationship</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for the relationship between agents in a society.</p> Source code in <code>src/aeiva/agent/relationship.py</code> <pre><code>class Relationship(ABC):\n\"\"\"\n    Abstract base class for the relationship between agents in a society.\n    \"\"\"\n    def __init__(self, agent1: 'Agent', agent2: 'Agent', relationship_type: str, relationship_strength: float, *args, **kwargs):\n        self._agent1 = agent1\n        self._agent2 = agent2\n        self._relationship_type = relationship_type\n        self._relationship_strength = relationship_strength\n\n    @property\n    def agents(self) -&gt; Tuple['Agent', 'Agent']:\n        return self._agent1, self._agent2\n\n    @property\n    def relationship_type(self) -&gt; str:\n        return self._relationship_type\n\n    @property\n    def relationship_strength(self) -&gt; float:\n        return self._relationship_strength\n\n    @relationship_strength.setter\n    def relationship_strength(self, new_strength: float):\n        self._relationship_strength = new_strength\n\n    @abstractmethod\n    def update_relationship(self, *args, **kwargs):\n        pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.role","title":"<code>role</code>","text":""},{"location":"reference/#src.aeiva.agent.role.Role","title":"<code>Role</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for the role of an agent in a society.</p> Source code in <code>src/aeiva/agent/role.py</code> <pre><code>class Role(ABC):\n\"\"\"\n    Abstract base class for the role of an agent in a society.\n    \"\"\"\n    def __init__(self, role_name: str, role_description: Optional[str] = None, *args, **kwargs):\n        self._role_name = role_name\n        self._role_description = role_description\n\n    @property\n    def role_name(self) -&gt; str:\n        return self._role_name\n\n    @property\n    def role_description(self) -&gt; Optional[str]:\n        return self._role_description\n\n    @abstractmethod\n    def define_role(self, *args, **kwargs):\n        pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.society","title":"<code>society</code>","text":""},{"location":"reference/#src.aeiva.agent.society.Society","title":"<code>Society</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for a society of agents.</p> Source code in <code>src/aeiva/agent/society.py</code> <pre><code>class Society(ABC):\n\"\"\"\n    Abstract base class for a society of agents.\n    \"\"\"\n\n    def __init__(self, society_name: str, society_description: Optional[str] = None, *args, **kwargs):\n        self._society_name = society_name\n        self._society_description = society_description\n        self._members = []  # List to store members of the society\n        self._norms = {}  # Dictionary to store societal norms/rules\n        self._relationships = {}  # Dictionary to store relationships between members\n\n    @property\n    def society_name(self) -&gt; str:\n        return self._society_name\n\n    @property\n    def society_description(self) -&gt; Optional[str]:\n        return self._society_description\n\n    @abstractmethod\n    def add_member(self, agent: 'Agent'):\n\"\"\"\n        Abstract method for adding an agent to the society. This should be implemented in a concrete subclass\n        according to the specific behavior of the society.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def remove_member(self, agent: 'Agent'):\n\"\"\"\n        Abstract method for removing an agent from the society. This should be implemented in a concrete subclass\n        according to the specific behavior of the society.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_relationships(self, agent: 'Agent') -&gt; list[Tuple['Agent', str, float]]:\n\"\"\"\n        Abstract method for getting the relationships of an agent in the society. This should return a list of tuples,\n        where each tuple contains an agent, the type of relationship, and the strength of the relationship. This should \n        be implemented in a concrete subclass according to the specific behavior of the society.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def set_norms(self, norms: dict[str, Any]):\n\"\"\"\n        Abstract method for setting societal norms/rules. This should be implemented in a concrete subclass according \n        to the specific behavior of the society.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.society.Society.add_member","title":"<code>add_member(agent)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for adding an agent to the society. This should be implemented in a concrete subclass according to the specific behavior of the society.</p> Source code in <code>src/aeiva/agent/society.py</code> <pre><code>@abstractmethod\ndef add_member(self, agent: 'Agent'):\n\"\"\"\n    Abstract method for adding an agent to the society. This should be implemented in a concrete subclass\n    according to the specific behavior of the society.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.society.Society.get_relationships","title":"<code>get_relationships(agent)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for getting the relationships of an agent in the society. This should return a list of tuples, where each tuple contains an agent, the type of relationship, and the strength of the relationship. This should  be implemented in a concrete subclass according to the specific behavior of the society.</p> Source code in <code>src/aeiva/agent/society.py</code> <pre><code>@abstractmethod\ndef get_relationships(self, agent: 'Agent') -&gt; list[Tuple['Agent', str, float]]:\n\"\"\"\n    Abstract method for getting the relationships of an agent in the society. This should return a list of tuples,\n    where each tuple contains an agent, the type of relationship, and the strength of the relationship. This should \n    be implemented in a concrete subclass according to the specific behavior of the society.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.society.Society.remove_member","title":"<code>remove_member(agent)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for removing an agent from the society. This should be implemented in a concrete subclass according to the specific behavior of the society.</p> Source code in <code>src/aeiva/agent/society.py</code> <pre><code>@abstractmethod\ndef remove_member(self, agent: 'Agent'):\n\"\"\"\n    Abstract method for removing an agent from the society. This should be implemented in a concrete subclass\n    according to the specific behavior of the society.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.society.Society.set_norms","title":"<code>set_norms(norms)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for setting societal norms/rules. This should be implemented in a concrete subclass according  to the specific behavior of the society.</p> Source code in <code>src/aeiva/agent/society.py</code> <pre><code>@abstractmethod\ndef set_norms(self, norms: dict[str, Any]):\n\"\"\"\n    Abstract method for setting societal norms/rules. This should be implemented in a concrete subclass according \n    to the specific behavior of the society.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.stimuli","title":"<code>stimuli</code>","text":""},{"location":"reference/#src.aeiva.agent.stimuli.Stimuli","title":"<code>Stimuli</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for stimuli. This could be any information that an agent can perceive from the environment. The actual process of generating sensory_data is left as an abstract method, allowing subclasses to define  the specific data generation.</p> Source code in <code>src/aeiva/agent/stimuli.py</code> <pre><code>class Stimuli(ABC):\n\"\"\"\n    Abstract base class for stimuli. This could be any information that an agent can perceive from the environment.\n    The actual process of generating sensory_data is left as an abstract method, allowing subclasses to define \n    the specific data generation.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        self.sensory_data = self.generate_sensory_data(*args, **kwargs)  # A dictionary containing various types of sensory data\n\n    @property\n    def sensory_data(self) -&gt; Dict[str, Any]:\n        return self._sensory_data\n\n    @sensory_data.setter\n    def sensory_data(self, value: Dict[str, Any]):\n        self._sensory_data = value\n\n    @abstractmethod\n    def generate_sensory_data(self, *args, **kwargs) -&gt; Dict[str, Any]:\n\"\"\"\n        Abstract method for generating the sensory_data. The actual generation should be defined in subclasses.\n        The output is stored in the sensory_data attribute.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.stimuli.Stimuli.generate_sensory_data","title":"<code>generate_sensory_data(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for generating the sensory_data. The actual generation should be defined in subclasses. The output is stored in the sensory_data attribute.</p> Source code in <code>src/aeiva/agent/stimuli.py</code> <pre><code>@abstractmethod\ndef generate_sensory_data(self, *args, **kwargs) -&gt; Dict[str, Any]:\n\"\"\"\n    Abstract method for generating the sensory_data. The actual generation should be defined in subclasses.\n    The output is stored in the sensory_data attribute.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.thought","title":"<code>thought</code>","text":""},{"location":"reference/#src.aeiva.agent.thought.Thought","title":"<code>Thought</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for thoughts. This represents the outcome of the agent's cognitive process, based on its observations.</p> Source code in <code>src/aeiva/agent/thought.py</code> <pre><code>class Thought(ABC):\n\"\"\"\n    Abstract base class for thoughts. This represents the outcome of the agent's cognitive process, based on its observations.\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        self.concepts, self.emotions = self.process_thought(*args, **kwargs)\n\n    @property\n    def emotions(self) -&gt; Dict[str, float]:\n        return self._emotions\n\n    @emotions.setter\n    def emotions(self, value: Dict[str, float]):\n        self._emotions = value\n\n    @abstractmethod\n    def process_thought(self, *args, **kwargs) -&gt; Tuple[Dict[str, Any], Dict[str, float]]:\n\"\"\"\n        Abstract method for processing the agent's thoughts. \n        Should return a dictionary of concepts the agent is considering, and a dictionary of emotional states associated with each concept.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.thought.Thought.process_thought","title":"<code>process_thought(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for processing the agent's thoughts.  Should return a dictionary of concepts the agent is considering, and a dictionary of emotional states associated with each concept.</p> Source code in <code>src/aeiva/agent/thought.py</code> <pre><code>@abstractmethod\ndef process_thought(self, *args, **kwargs) -&gt; Tuple[Dict[str, Any], Dict[str, float]]:\n\"\"\"\n    Abstract method for processing the agent's thoughts. \n    Should return a dictionary of concepts the agent is considering, and a dictionary of emotional states associated with each concept.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.tool","title":"<code>tool</code>","text":""},{"location":"reference/#src.aeiva.agent.tool.Tool","title":"<code>Tool</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for a tool that an agent can use.</p> Source code in <code>src/aeiva/agent/tool.py</code> <pre><code>class Tool(ABC):\n\"\"\"\n    Abstract base class for a tool that an agent can use.\n    \"\"\"\n    def __init__(self, tool_name: str, tool_description: Optional[str] = None, *args, **kwargs):\n        self._tool_name = tool_name\n        self._tool_description = tool_description\n\n    @property\n    def tool_name(self) -&gt; str:\n        return self._tool_name\n\n    @property\n    def tool_description(self) -&gt; Optional[str]:\n        return self._tool_description\n\n    @abstractmethod\n    def use_tool(self, *args, **kwargs):\n\"\"\"\n        Abstract method for using the tool. This should be implemented in a concrete subclass\n        according to the specific behavior of the tool when used.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.tool.Tool.use_tool","title":"<code>use_tool(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Abstract method for using the tool. This should be implemented in a concrete subclass according to the specific behavior of the tool when used.</p> Source code in <code>src/aeiva/agent/tool.py</code> <pre><code>@abstractmethod\ndef use_tool(self, *args, **kwargs):\n\"\"\"\n    Abstract method for using the tool. This should be implemented in a concrete subclass\n    according to the specific behavior of the tool when used.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.world_model","title":"<code>world_model</code>","text":""},{"location":"reference/#src.aeiva.agent.world_model.WorldModel","title":"<code>WorldModel</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for the world model of an agent. This class represents the agent's internal representation and understanding of the world.</p> Source code in <code>src/aeiva/agent/world_model.py</code> <pre><code>class WorldModel(ABC):\n\"\"\"\n    Abstract base class for the world model of an agent.\n    This class represents the agent's internal representation and understanding of the world.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self, *args, **kwargs):\n\"\"\"\n        Initialize a new WorldModel instance.\n        This should set up any necessary data structures and state variables.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update(self, new_information, *args, **kwargs):\n\"\"\"\n        Update the world model with new information.\n        This could involve modifying the internal data structure, reevaluating beliefs, etc.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def query(self, query_type: str, *args, **kwargs):\n\"\"\"\n        Query the world model based on a specified type and additional arguments.\n        This could involve searching the internal data structure, performing inference, etc.\n        The query_type argument specifies the type of the query, and additional arguments may be required depending on the query type.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def visualize(self, visualization_type: str, *args, **kwargs):\n\"\"\"\n        Generate a visualization of the world model based on a specified type and additional arguments.\n        This could be useful for understanding the agent's current state of knowledge.\n        The visualization_type argument specifies the type of the visualization, and additional arguments may be required depending on the visualization type.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def self_organize(self, *args, **kwargs):\n\"\"\"\n        Allow the world model to self-organize based on its current state and any additional arguments.\n        This could involve restructuring the internal data structure, updating beliefs, etc.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def predict(self, future_steps: int, *args, **kwargs) -&gt; Any:\n\"\"\"\n        Make a prediction about the state of the world some number of steps into the future.\n        The prediction could be based on the current state of the world model and any additional arguments.\n        The method should return the prediction.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def reset(self):\n\"\"\"\n        Reset the world model to its default state.\n        This could involve clearing the internal data structure, resetting beliefs, etc.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.world_model.WorldModel.__init__","title":"<code>__init__(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Initialize a new WorldModel instance. This should set up any necessary data structures and state variables.</p> Source code in <code>src/aeiva/agent/world_model.py</code> <pre><code>@abstractmethod\ndef __init__(self, *args, **kwargs):\n\"\"\"\n    Initialize a new WorldModel instance.\n    This should set up any necessary data structures and state variables.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.world_model.WorldModel.predict","title":"<code>predict(future_steps, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Make a prediction about the state of the world some number of steps into the future. The prediction could be based on the current state of the world model and any additional arguments. The method should return the prediction.</p> Source code in <code>src/aeiva/agent/world_model.py</code> <pre><code>@abstractmethod\ndef predict(self, future_steps: int, *args, **kwargs) -&gt; Any:\n\"\"\"\n    Make a prediction about the state of the world some number of steps into the future.\n    The prediction could be based on the current state of the world model and any additional arguments.\n    The method should return the prediction.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.world_model.WorldModel.query","title":"<code>query(query_type, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Query the world model based on a specified type and additional arguments. This could involve searching the internal data structure, performing inference, etc. The query_type argument specifies the type of the query, and additional arguments may be required depending on the query type.</p> Source code in <code>src/aeiva/agent/world_model.py</code> <pre><code>@abstractmethod\ndef query(self, query_type: str, *args, **kwargs):\n\"\"\"\n    Query the world model based on a specified type and additional arguments.\n    This could involve searching the internal data structure, performing inference, etc.\n    The query_type argument specifies the type of the query, and additional arguments may be required depending on the query type.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.world_model.WorldModel.reset","title":"<code>reset()</code>  <code>abstractmethod</code>","text":"<p>Reset the world model to its default state. This could involve clearing the internal data structure, resetting beliefs, etc.</p> Source code in <code>src/aeiva/agent/world_model.py</code> <pre><code>@abstractmethod\ndef reset(self):\n\"\"\"\n    Reset the world model to its default state.\n    This could involve clearing the internal data structure, resetting beliefs, etc.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.world_model.WorldModel.self_organize","title":"<code>self_organize(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Allow the world model to self-organize based on its current state and any additional arguments. This could involve restructuring the internal data structure, updating beliefs, etc.</p> Source code in <code>src/aeiva/agent/world_model.py</code> <pre><code>@abstractmethod\ndef self_organize(self, *args, **kwargs):\n\"\"\"\n    Allow the world model to self-organize based on its current state and any additional arguments.\n    This could involve restructuring the internal data structure, updating beliefs, etc.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.world_model.WorldModel.update","title":"<code>update(new_information, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Update the world model with new information. This could involve modifying the internal data structure, reevaluating beliefs, etc.</p> Source code in <code>src/aeiva/agent/world_model.py</code> <pre><code>@abstractmethod\ndef update(self, new_information, *args, **kwargs):\n\"\"\"\n    Update the world model with new information.\n    This could involve modifying the internal data structure, reevaluating beliefs, etc.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.agent.world_model.WorldModel.visualize","title":"<code>visualize(visualization_type, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Generate a visualization of the world model based on a specified type and additional arguments. This could be useful for understanding the agent's current state of knowledge. The visualization_type argument specifies the type of the visualization, and additional arguments may be required depending on the visualization type.</p> Source code in <code>src/aeiva/agent/world_model.py</code> <pre><code>@abstractmethod\ndef visualize(self, visualization_type: str, *args, **kwargs):\n\"\"\"\n    Generate a visualization of the world model based on a specified type and additional arguments.\n    This could be useful for understanding the agent's current state of knowledge.\n    The visualization_type argument specifies the type of the visualization, and additional arguments may be required depending on the visualization type.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/#src.aeiva.common","title":"<code>common</code>","text":""},{"location":"reference/#src.aeiva.common.decorators","title":"<code>decorators</code>","text":""},{"location":"reference/#src.aeiva.common.decorators.import_submodules","title":"<code>import_submodules(package, recursive=True)</code>","text":"<p>Import all submodules of a module, recursively, including subpackages</p> Source code in <code>src/aeiva/common/decorators.py</code> <pre><code>def import_submodules(package, recursive=True):\n\"\"\" Import all submodules of a module, recursively, including subpackages \"\"\"\n\n    if isinstance(package, str):\n        package = importlib.import_module(package)\n\n    results = {}\n\n    for loader, name, is_pkg in pkgutil.walk_packages(package.__path__):\n        full_name = package.__name__ + \".\" + name\n        results[full_name] = importlib.import_module(full_name)\n        if recursive and is_pkg:\n            results.update(import_submodules(full_name))\n\n    return results\n</code></pre>"},{"location":"reference/#src.aeiva.common.id_generator","title":"<code>id_generator</code>","text":""},{"location":"reference/#src.aeiva.common.id_generator.IDGenerator","title":"<code>IDGenerator</code>","text":"<p>A simple class to generate unique IDs for distinct names.</p> <p>Attributes:</p> Name Type Description <code>name_to_id</code> <code>dict</code> <p>A dictionary to map names to IDs.</p> <code>next_id</code> <code>int</code> <p>The next ID to be assigned.</p> Source code in <code>src/aeiva/common/id_generator.py</code> <pre><code>class IDGenerator:\n\"\"\"\n    A simple class to generate unique IDs for distinct names.\n\n    Attributes:\n        name_to_id (dict): A dictionary to map names to IDs.\n        next_id (int): The next ID to be assigned.\n    \"\"\"\n\n    def __init__(self):\n\"\"\"\n        Constructs all the necessary attributes for the IDGenerator object.\n\n        Attributes:\n            name_to_id (dict): Initializes an empty dictionary to map names to IDs.\n            next_id (int): Initializes the next ID to be assigned as 0.\n        \"\"\"\n        self.name_to_id = {}\n        self.next_id = 0\n\n    def get_id(self, name: str) -&gt; int:\n\"\"\"\n        Returns the ID of the 'name'. If 'name' does not exist, assigns a new ID.\n\n        Parameters:\n            name (str): The name for which the ID is required.\n\n        Returns:\n            int: The ID associated with the 'name'.\n        \"\"\"\n        if name not in self.name_to_id:\n            self.name_to_id[name] = self.next_id\n            self.next_id += 1\n        return self.name_to_id[name]\n</code></pre>"},{"location":"reference/#src.aeiva.common.id_generator.IDGenerator.__init__","title":"<code>__init__()</code>","text":"<p>Constructs all the necessary attributes for the IDGenerator object.</p> <p>Attributes:</p> Name Type Description <code>name_to_id</code> <code>dict</code> <p>Initializes an empty dictionary to map names to IDs.</p> <code>next_id</code> <code>int</code> <p>Initializes the next ID to be assigned as 0.</p> Source code in <code>src/aeiva/common/id_generator.py</code> <pre><code>def __init__(self):\n\"\"\"\n    Constructs all the necessary attributes for the IDGenerator object.\n\n    Attributes:\n        name_to_id (dict): Initializes an empty dictionary to map names to IDs.\n        next_id (int): Initializes the next ID to be assigned as 0.\n    \"\"\"\n    self.name_to_id = {}\n    self.next_id = 0\n</code></pre>"},{"location":"reference/#src.aeiva.common.id_generator.IDGenerator.get_id","title":"<code>get_id(name)</code>","text":"<p>Returns the ID of the 'name'. If 'name' does not exist, assigns a new ID.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name for which the ID is required.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The ID associated with the 'name'.</p> Source code in <code>src/aeiva/common/id_generator.py</code> <pre><code>def get_id(self, name: str) -&gt; int:\n\"\"\"\n    Returns the ID of the 'name'. If 'name' does not exist, assigns a new ID.\n\n    Parameters:\n        name (str): The name for which the ID is required.\n\n    Returns:\n        int: The ID associated with the 'name'.\n    \"\"\"\n    if name not in self.name_to_id:\n        self.name_to_id[name] = self.next_id\n        self.next_id += 1\n    return self.name_to_id[name]\n</code></pre>"},{"location":"reference/#src.aeiva.common.pipeline","title":"<code>pipeline</code>","text":""},{"location":"reference/#src.aeiva.common.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>This class is used to rurn a list of functions into a pipeline.</p> Source code in <code>src/aeiva/common/pipeline.py</code> <pre><code>class Pipeline:\nr\"\"\"This class is used to rurn a list of functions into a pipeline.\"\"\"\n    def __init__(self, functions):\n        self.functions = functions\n\n    def run(self, *args, **kwargs):\n        result = self.functions[0](*args, **kwargs)\n        for f in self.functions[1:]:\n            if isinstance(result, tuple):\n                result = f(*result)\n            else:\n                result = f(result)\n        return result\n\n    def __call__(self, *args, **kwargs):\n        return self.run(*args, **kwargs)\n</code></pre>"},{"location":"reference/#src.aeiva.common.types","title":"<code>types</code>","text":""},{"location":"reference/#src.aeiva.common.types.DataBatch","title":"<code>DataBatch</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>DataBatch is a batch of data items created by a dataloader.</p> Source code in <code>src/aeiva/common/types.py</code> <pre><code>class DataBatch(TypedDict):\nr\"\"\"DataBatch is a batch of data items created by a dataloader.\n    \"\"\"\n    videos: Optional[torch.Tensor]  # videos representation\n    audios: Optional[torch.Tensor]  # audios representation\n    images: Optional[torch.Tensor]  # images representation\n    input_ids: Optional[torch.Tensor]  # text token ids\n    attention_mask: Optional[torch.Tensor]  # attention mask\n    image_starts: Optional[torch.Tensor]  # image start token\n    image_ends: Optional[torch.Tensor]  # image end token\n    audio_starts: Optional[torch.Tensor]  # audio start token\n    audio_ends: Optional[torch.Tensor]  # audio end token\n    video_starts: Optional[torch.Tensor]  # video start token\n    video_ends: Optional[torch.Tensor]  # video end token\n    labels: Optional[torch.Tensor]  # labels\n</code></pre>"},{"location":"reference/#src.aeiva.common.types.DataItem","title":"<code>DataItem</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>DataItem is a dictionary that contains all the information for a single data item.</p> Source code in <code>src/aeiva/common/types.py</code> <pre><code>class DataItem(TypedDict):\nr\"\"\"DataItem is a dictionary that contains all the information for a single data item.\n    \"\"\"\n    instruction: str  # instruction text\n    input: Optional[str]  # input text\n    output: Optional[str]  # output text\n    text: Optional[str]  # text field. How it is formed depends on the task.\n\n    image: Optional[str]  # image name or path\n    transformed_image: Optional[torch.Tensor]  # transformed image tensor\n\n    audio: Optional[str]  # audio name or path\n    audio_mels: Optional[torch.Tensor]  # audio melspectrogram tensor\n\n    video: Optional[str]  # video name or path\n    sampled_video_frame_indices: Optional[list[int]]  # sampled video frame indices\n    video_frames: Optional[torch.Tensor]  # video frames tensor\n</code></pre>"},{"location":"reference/#src.aeiva.common.types.DataSet","title":"<code>DataSet</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>DataSet is a dictionary that contains data items and meta information.</p> Source code in <code>src/aeiva/common/types.py</code> <pre><code>class DataSet(TypedDict):\nr\"\"\"DataSet is a dictionary that contains data items and meta information.\n    \"\"\"\n    data: list[DataItem]\n    metadata: dict[str, Any]\n</code></pre>"},{"location":"reference/#src.aeiva.common.types.TaskContext","title":"<code>TaskContext</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>TaskContext is a dictionary that contains all the information for a task.</p> Source code in <code>src/aeiva/common/types.py</code> <pre><code>class TaskContext(TypedDict):\nr\"\"\"TaskContext is a dictionary that contains all the information for a task.\n    \"\"\"\n    config_path: Optional[str]\n    config: Optional[OmniConfig]\n    dataloader: Optional[torch.utils.data.DataLoader]\n    tokenizer: Optional[Any]\n    model: Optional[Any]\n    logger: Optional[Any]\n    trainer: Optional[Any]\n    current_model_input: Optional[DataItem]\n    current_model_output: Optional[Any]\n</code></pre>"},{"location":"reference/#src.aeiva.config","title":"<code>config</code>","text":""},{"location":"reference/#src.aeiva.config.base_config","title":"<code>base_config</code>","text":"<p>This module contains the base config classes.</p> <p>We can define separate config classes for different modules, e.g., data, model, trainer, etc. They will be automatically registered in the BaseConfig class.</p> <p>Copyright (C) 2023 Bang Liu - All Rights Reserved. This source code is licensed under the license found in the LICENSE file in the root directory of this source tree.</p>"},{"location":"reference/#src.aeiva.config.custom_configs","title":"<code>custom_configs</code>","text":""},{"location":"reference/#src.aeiva.config.custom_configs.macaw_config","title":"<code>macaw_config</code>","text":"<p>This module contains the config for macaw model.</p> <p>We can define separate config classes for different specific models/datasets/tasks.</p> <p>Copyright (C) 2023 Bang Liu - All Rights Reserved. This source code is licensed under the license found in the LICENSE file in the root directory of this source tree.</p>"},{"location":"reference/#src.aeiva.config.custom_configs.macaw_config.MacawConfig","title":"<code>MacawConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>BaseConfig</code></p> <p>Define user-customized config here.</p> Source code in <code>src/aeiva/config/custom_configs/macaw_config.py</code> <pre><code>@dataclass\nclass MacawConfig(BaseConfig):\n\"\"\"\n    Define user-customized config here.\n    \"\"\"\n    image_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The directory of image data\"}\n    )\n    video_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The directory of video data\"}\n    )\n    frame_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The directory to save video frames\"}\n    )\n    audio_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The directory to save video audios\"}\n    )\n    num_frames_to_sample: Optional[int] = field(\n        default=120,\n        metadata={\"help\": \"The number of frames to sample from a video\"}\n    )\n    num_frames_to_load: Optional[int] = field(\n        default=6,\n        metadata={\"help\": \"The number of frames to load as a part of model inputs\"}\n    )\n    num_samples_per_dataset: Optional[int] = field(\n        default=100,\n        metadata={\"help\": \"The number of samples to load from each dataset\"}\n    )\n    num_samples_per_merged_dataset: Optional[int] = field(\n        default=20,\n        metadata={\"help\": \"The number of samples to save after merging datasets\"}\n    )\n    batch_size: Optional[int] = field(\n        default=1,\n        metadata={\"help\": \"The batch size of model inputs\"}\n    )\n    max_seq_len_for_preprocess: Optional[int] = field(\n        default=256,\n        metadata={\"help\": \"The maximum sequence length for preprocess\"}\n    )\n    run_time_cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The directory to save running time data, such as video frames, audios, and so on.\"}\n    )\n    tokenizer_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The name or path of tokenizer\"}\n    )\n    clip_model_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The name or path of clip model\"}\n    )\n    whisper_model_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The name or path of whisper model\"}\n    )\n    llama7b_model_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The name or path of llama7b model\"}\n    )\n    macaw_model_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The name or path of macaw model\"}\n    )\n    mode: Optional[str] = field(\n        default=\"train\",\n        metadata={\"help\": \"The mode of train, eval, or inference\"}\n    )\n    model_name: Optional[str] = field(\n        default=\"macaw\",\n        metadata={\"help\": \"The name of model\"}\n    )\n    resource_ready: Optional[bool] = field(\n        default=True,\n        metadata={\"help\": \"Whether the pre-requisite resource is ready, e.g., download pretrained models and datasets\"}\n    )\n</code></pre>"},{"location":"reference/#src.aeiva.config.general_configs","title":"<code>general_configs</code>","text":"<p>This module contains some general config classes that can be used in deep learning projects.</p> <p>E.g., data config, model config, trainer config, etc.</p> <p>Copyright (C) 2023 Bang Liu - All Rights Reserved. This source code is licensed under the license found in the LICENSE file in the root directory of this source tree.</p>"},{"location":"reference/#src.aeiva.config.general_configs.DataConfig","title":"<code>DataConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>BaseConfig</code></p> <p>This class contains the data configuration.</p> Source code in <code>src/aeiva/config/general_configs.py</code> <pre><code>@dataclass\nclass DataConfig(BaseConfig):\n\"\"\"This class contains the data configuration.\"\"\"\n    dataset_path: Optional[str] = field(\n        default=None, metadata={\"help\": \"The path of the dataset to use.\"}\n    )\n    dataset_name: Optional[str] = field(\n        default=\"customized\", metadata={\"help\": \"Should be \\\"customized\\\"\"}\n    )\n    is_custom_dataset: Optional[bool] = field(\n        default=False, metadata={\"help\": \"whether to use custom data\"}\n    )\n    customized_cache_dir: Optional[str] = field(\n        default=\".cache/llm-ft/datasets\",\n        metadata={\"help\": \"Where do you want to store the customized dataset caches\"},\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n    validation_file: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n    )\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=1e10,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    streaming: Optional[bool] = field(default=False, metadata={\"help\": \"Enable streaming mode\"})\n    block_size: Optional[int] = field(\n        default=512,\n        metadata={\n            \"help\": (\n                \"Optional input sequence length after tokenization. \"\n                \"The training dataset will be truncated in block of this size for training. \"\n                \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n            )\n        },\n    )\n    overwrite_cache: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    validation_split_percentage: Optional[int] = field(\n        default=5,\n        metadata={\n            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n        },\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    group_texts_batch_size: Optional[int] = field(\n        default=1000,\n        metadata={\n            \"help\": (\n                \"Number of samples that will be grouped together to go though\"\n                \" `group_texts` operation. See `--disable_group_texts` for\"\n                \" detailed explanation of this operation.\"\n            )\n        }\n    )\n    disable_group_texts: Optional[bool] = field(\n        default=False,\n        metadata={\n            \"help\": (\n                \"Whether we group original samples together to generate sample\"\n                \" sequences of length `block_size`. By default, we group every\"\n                \" 1000 tokenized sequences together, divide them into \"\n                \" [{total_num_tokens} / {block_size}] sequences, each with\"\n                \" `block_size` tokens (the remaining tokens are ommited.\"\n                \" If this flag is set to True, we only group 1 tokenized\"\n                \" sequence, i.e. cutting long sequence into chunks.\"\n            )\n        },\n    )\n    keep_linebreaks: Optional[bool] = field(\n        default=True,\n        metadata={\"help\": \"Whether to keep line breaks when using TXT files or not.\"}\n    )\n    test_file: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Evaluation File Path\"},\n    )\n\n    def __post_init__(self):\n        if self.streaming:\n            require_version(\"datasets&gt;=2.0.0\", \"The streaming feature requires `datasets&gt;=2.0.0`\")\n\n        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n        else:\n            if self.train_file is not None:\n                extension = self.train_file.split(\".\")[-1]\n                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n            if self.validation_file is not None:\n                extension = self.validation_file.split(\".\")[-1]\n                assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, a json or a txt file.\"\n</code></pre>"},{"location":"reference/#src.aeiva.config.general_configs.ExplicitEnum","title":"<code>ExplicitEnum</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum with more explicit error message for missing values.</p> Source code in <code>src/aeiva/config/general_configs.py</code> <pre><code>class ExplicitEnum(str, Enum):\n\"\"\"\n    Enum with more explicit error message for missing values.\n    \"\"\"\n    @classmethod\n    def _missing_(cls, value):\n        raise ValueError(\n            f\"{value} is not a valid {cls.__name__}, please select one of {list(cls._value2member_map_.keys())}\"\n        )\n</code></pre>"},{"location":"reference/#src.aeiva.config.general_configs.ModelConfig","title":"<code>ModelConfig</code>  <code>dataclass</code>","text":"<p>             Bases: <code>BaseConfig</code></p> <p>Model configuration class.</p> Source code in <code>src/aeiva/config/general_configs.py</code> <pre><code>@dataclass\nclass ModelConfig(BaseConfig):\n\"\"\"Model configuration class.\"\"\"\n    model_name_or_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n            )\n        },\n    )\n    lora_model_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The incremental model diff introduced by LoRA finetuning.\"\n                \" Along with the original non-finetuned model forms the whole\"\n                \" finetuned model.\"\n            )\n        }\n    )\n    model_type: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n    )\n    arch_type: Optional[str] = field(\n        default=\"decoder_only\",\n        metadata={\"help\": \"The architecture type of the model. Currently supported decoder_only or encoder_decoder\"}\n    )\n    config_overrides: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Override some existing default config settings when a model is trained from scratch. Example: \"\n                \"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index\"\n            )\n        },\n    )\n    arch_type: Optional[str] = field(\n        default=\"decoder_only\",\n        metadata={\n            \"help\": (\n                \"Model architecture type, e.g. \\\"decoder_only\\\",\"\n                \" \\\"encoder_decoder\\\"\"\n            ),\n            \"choices\": [\"decoder_only\", \"encoder_decoder\", \"text_regression\", \"vision_encoder_decoder\"],\n        },\n    )\n    config_name: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: Optional[bool] = field(\n        default=True,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    model_revision: Optional[str] = field(\n        default=\"main\",\n        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n    )\n    use_auth_token: Optional[bool] = field(\n        default=False,\n        metadata={\n            \"help\": (\n                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n                \"with private models).\"\n            )\n        },\n    )\n    torch_dtype: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Override the default `torch.dtype` and load the model under this dtype. If `auto` is passed, the \"\n                \"dtype will be automatically derived from the model's weights.\"\n            ),\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    use_lora: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Whether to lora.\"},\n    )\n    lora_r: Optional[int] = field(\n        default=8,\n        metadata={\"help\": \"the rank of the lora parameters. The smaller lora_r is , the fewer parameters lora has.\"},\n    )\n    lora_alpha: Optional[int] = field(\n        default=32,\n        metadata={\"help\": \"Merging ratio between the fine-tuned model and the original. This is controlled by a parameter called alpha in the paper.\"},\n    )\n    lora_target_modules: Optional[list[str]] = field(\n        default=None,\n        metadata={\"help\": \"Pretrained config name or path if not the same as model_name\",\n                              }\n    )\n    lora_dropout: Optional[float] = field(\n        default=0.1,\n        metadata={\"help\": \"The dropout rate in lora.linear.\"},\n    )\n    save_aggregated_lora: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Whether to save aggregated lora.\"},\n        )\n    use_ram_optimized_load: Optional[bool] = field(\n        default=True,\n        metadata={\"help\": \"Whether use disk mapping when memory is not enough.\"}\n    )\n    use_flash_attention: Optional[bool] = field(\n        default=False,\n        metadata={\n            \"help\": (\n                \"whether use flash attention layer to reduce GPU memory with\"\n                \" higher time cost.\"\n            )\n        }\n    )\n    use_int8: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"whether to load int8 quantization for inference\"}\n    )\n    custom_model: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"flag for the model from huggingface or not\"}\n    )\n    # below is added for macaw model\n    n_frames: Optional[int] = field(\n        default=6,\n        metadata={\n            \"help\": \"The number of frames for encoding a video.\"\n        },\n    )\n    attention_heads: Optional[int] = field(\n        default=220,\n        metadata={\n            \"help\": \"The number of attention heads used in multi-head-attention.\"\n        },\n    )\n    image_conv_kernel: Optional[int] = field(\n        default=48,\n        metadata={\n            \"help\": \"The size of the convolutional kernel for the image stream.\"\n        },\n    )\n    image_conv_stride: Optional[int] = field(\n        default=36,\n        metadata={\n            \"help\": \"The stride of the convolutional kernel for the image stream.\"\n        },\n    )\n    video_conv_kernel: Optional[int] = field(\n        default=36,\n        metadata={\n            \"help\": \"The size of the convolutional kernel for the video stream.\"\n        },\n    )\n    video_conv_stride: Optional[int] = field(\n        default=30,\n        metadata={\n            \"help\": \"The stride of the convolutional kernel for the video stream.\"\n        },\n    )\n    audio_conv_kernel: Optional[int] = field(\n        default=240,\n        metadata={\n            \"help\": \"The size of the convolutional kernel for the audio stream.\"\n        },\n    )\n    audio_conv_stride: Optional[int] = field(\n        default=220,\n        metadata={\n            \"help\": \"The stride of the convolutional kernel for the audio stream.\"\n        },\n    )\n    freeze_multi_modal_encoder: bool = field(\n        default=False,\n        metadata={\n            \"help\": (\n                \"Whether to freeze the parameters of multi-modal encoders during training.).\"\n            )\n        },\n    )\n\n    def __post_init__(self):\n        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):\n            raise ValueError(\n                \"--config_overrides can't be used in combination with --config_name or --model_name_or_path\"\n            )\n</code></pre>"},{"location":"reference/#src.aeiva.config.general_configs.OptimizerNames","title":"<code>OptimizerNames</code>","text":"<p>             Bases: <code>ExplicitEnum</code></p> <p>Stores the acceptable string identifiers for optimizers.</p> Source code in <code>src/aeiva/config/general_configs.py</code> <pre><code>class OptimizerNames(ExplicitEnum):\n\"\"\"\n    Stores the acceptable string identifiers for optimizers.\n    \"\"\"\n    ADAMW_HF = \"adamw_hf\"\n    ADAMW_TORCH = \"adamw_torch\"\n    ADAMW_TORCH_FUSED = \"adamw_torch_fused\"\n    ADAMW_TORCH_XLA = \"adamw_torch_xla\"\n    ADAMW_APEX_FUSED = \"adamw_apex_fused\"\n    ADAFACTOR = \"adafactor\"\n    ADAMW_ANYPRECISION = \"adamw_anyprecision\"\n    SGD = \"sgd\"\n    ADAGRAD = \"adagrad\"\n    ADAMW_BNB = \"adamw_bnb_8bit\"\n    ADAMW_8BIT = \"adamw_8bit\"  # just an alias for adamw_bnb_8bit\n    LION_8BIT = \"lion_8bit\"\n    LION = \"lion_32bit\"\n    PAGED_ADAMW = \"paged_adamw_32bit\"\n    PAGED_ADAMW_8BIT = \"paged_adamw_8bit\"\n    PAGED_LION = \"paged_lion_32bit\"\n    PAGED_LION_8BIT = \"paged_lion_8bit\"\n</code></pre>"},{"location":"reference/#src.aeiva.config.omni_config","title":"<code>omni_config</code>","text":"<p>This module contains the omniconfig classes.</p> <p>We can define separate config classes for different modules, e.g., data, model, trainer, etc. The OmniConfig class is the combination of all config classes. It can also accept command line arguments to update the config values.</p> <p>Copyright (C) 2023 Bang Liu - All Rights Reserved. This source code is licensed under the license found in the LICENSE file in the root directory of this source tree.</p>"},{"location":"reference/#src.aeiva.data","title":"<code>data</code>","text":""},{"location":"reference/#src.aeiva.data.processor","title":"<code>processor</code>","text":"<p>This module contains the data processor.</p> <p>@Author: Bang Liu (chatsci.ai@gmail.com) @Date: 2023-07-11</p> <p>Copyright (C) 2023 Bang Liu - All Rights Reserved. This source code is licensed under the license found in the LICENSE file in the root directory of this source tree.</p>"},{"location":"reference/#src.aeiva.data.processor.process_dataset","title":"<code>process_dataset(formatted_dataset, pipeline, output_dir, dataset_name='')</code>","text":"<p>Process a dataset with a pipeline of functions.</p> <p>Parameters:</p> Name Type Description Default <code>formatted_dataset</code> <code>DataSet</code> <p>the dataset to be processed.</p> required <code>pipeline</code> <code>list[Callable]</code> <p>a list of functions to be applied to the dataset.</p> required <code>output_dir</code> <code>Optional[str]</code> <p>the output directory to save the processed dataset.</p> required <code>dataset_name</code> <code>Optional[str]</code> <p>the name of the dataset. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>DataSet</code> <code>DataSet</code> <p>the processed dataset.</p> Source code in <code>src/aeiva/data/processor.py</code> <pre><code>def process_dataset(formatted_dataset: DataSet,\n                    pipeline: list[Callable],\n                    output_dir: Optional[str],\n                    dataset_name: Optional[str] = \"\") -&gt; DataSet:\n\"\"\"\n    Process a dataset with a pipeline of functions.\n\n    Args:\n        formatted_dataset (DataSet): the dataset to be processed.\n        pipeline (list[Callable]): a list of functions to be applied to the dataset.\n        output_dir (Optional[str]): the output directory to save the processed dataset.\n        dataset_name (Optional[str], optional): the name of the dataset. Defaults to \"\".\n\n    Returns:\n        DataSet: the processed dataset.\n    \"\"\"\n    processed_data = []\n    pipeline = Pipeline(pipeline)\n    for item in formatted_dataset[\"data\"]:\n        processed_data.append(pipeline(item.copy()))\n\n    output = {\"data\": processed_data, \"metadata\": formatted_dataset[\"metadata\"]}\n    if output_dir is not None:\n        ensure_dir(output_dir)\n        dump_json(output, f\"{output_dir}/{dataset_name}_dataset.processed.json\")\n    return output\n</code></pre>"},{"location":"reference/#src.aeiva.demo","title":"<code>demo</code>","text":""},{"location":"reference/#src.aeiva.demo.mm_chatbot","title":"<code>mm_chatbot</code>","text":"<p>This module defines a multimodal chatbot demo with gradio.</p> <p>@Author: Bang Liu (chatsci.ai@gmail.com) @Date: 2023-07-13</p> <p>Copyright (C) 2023 Bang Liu - All Rights Reserved. This source code is licensed under the license found in the LICENSE file in the root directory of this source tree.</p>"},{"location":"reference/#src.aeiva.model","title":"<code>model</code>","text":""},{"location":"reference/#src.aeiva.model.macaw_model","title":"<code>macaw_model</code>","text":""},{"location":"reference/#src.aeiva.model.macaw_model.LlamaAttention","title":"<code>LlamaAttention</code>","text":"<p>             Bases: <code>nn.Module</code></p> <p>Multi-headed attention from 'Attention Is All You Need' paper</p> Source code in <code>src/aeiva/model/macaw_model.py</code> <pre><code>class LlamaAttention(nn.Module):\n\"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.max_position_embeddings = config.max_position_embeddings\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n        self.rotary_emb = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n    ) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n        # [bsz, nh, t, hd]\n\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model.LlamaDecoderLayer","title":"<code>LlamaDecoderLayer</code>","text":"<p>             Bases: <code>nn.Module</code></p> Source code in <code>src/aeiva/model/macaw_model.py</code> <pre><code>class LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.self_attn = LlamaAttention(config=config)\n        self.mlp = LlamaMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n        )\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n    ) -&gt; Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n\"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model.LlamaDecoderLayer.forward","title":"<code>forward(hidden_states, attention_mask=None, position_ids=None, past_key_value=None, output_attentions=False, use_cache=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>hidden_states</code> <code>`torch.FloatTensor`</code> <p>input to the layer of shape <code>(batch, seq_len, embed_dim)</code></p> required <code>attention_mask</code> <code>`torch.FloatTensor`, *optional*</code> <p>attention mask of size <code>(batch, 1, tgt_len, src_len)</code> where padding elements are indicated by very large negative values.</p> <code>None</code> <code>output_attentions</code> <code>`bool`, *optional*</code> <p>Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned tensors for more detail.</p> <code>False</code> <code>use_cache</code> <code>`bool`, *optional*</code> <p>If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see <code>past_key_values</code>).</p> <code>False</code> <code>past_key_value</code> <code>`Tuple(torch.FloatTensor)`, *optional*</code> <p>cached past key and value projection states</p> <code>None</code> Source code in <code>src/aeiva/model/macaw_model.py</code> <pre><code>def forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: Optional[bool] = False,\n    use_cache: Optional[bool] = False,\n) -&gt; Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n\"\"\"\n    Args:\n        hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n        attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n            `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n            returned tensors for more detail.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n            (see `past_key_values`).\n        past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n    \"\"\"\n\n    residual = hidden_states\n\n    hidden_states = self.input_layernorm(hidden_states)\n\n    # Self Attention\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n        hidden_states=hidden_states,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        past_key_value=past_key_value,\n        output_attentions=output_attentions,\n        use_cache=use_cache,\n    )\n    hidden_states = residual + hidden_states\n\n    # Fully Connected\n    residual = hidden_states\n    hidden_states = self.post_attention_layernorm(hidden_states)\n    hidden_states = self.mlp(hidden_states)\n    hidden_states = residual + hidden_states\n\n    outputs = (hidden_states,)\n\n    if output_attentions:\n        outputs += (self_attn_weights,)\n\n    if use_cache:\n        outputs += (present_key_value,)\n\n    return outputs\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model.LlamaModel","title":"<code>LlamaModel</code>","text":"<p>             Bases: <code>LlamaPreTrainedModel</code></p> <p>Transformer decoder consisting of config.num_hidden_layers layers. Each layer is a [<code>LlamaDecoderLayer</code>]</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LlamaConfig</code> <p>LlamaConfig</p> required Source code in <code>src/aeiva/model/macaw_model.py</code> <pre><code>class LlamaModel(LlamaPreTrainedModel):\n\"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n        # create causal mask\n        # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]\n        combined_attention_mask = None\n        if input_shape[-1] &gt; 1:\n            combined_attention_mask = _make_causal_mask(\n                input_shape,\n                inputs_embeds.dtype,\n                device=inputs_embeds.device,\n                past_key_values_length=past_key_values_length,\n            )\n\n        if attention_mask is not None:\n            # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]\n            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n                inputs_embeds.device\n            )\n            combined_attention_mask = (\n                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n            )\n\n        return combined_attention_mask\n\n    # @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -&gt; Union[Tuple, BaseModelOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # retrieve input_ids and inputs_embeds\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n        elif input_ids is not None:\n            batch_size, seq_length = input_ids.shape\n        elif inputs_embeds is not None:\n            batch_size, seq_length, _ = inputs_embeds.shape\n        else:\n            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n\n        seq_length_with_past = seq_length\n        past_key_values_length = 0\n\n        if past_key_values is not None:\n            past_key_values_length = past_key_values[0][0].shape[2]\n            seq_length_with_past = seq_length_with_past + past_key_values_length\n\n        if position_ids is None:\n            device = input_ids.device if input_ids is not None else inputs_embeds.device\n            position_ids = torch.arange(\n                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n            )\n            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n        else:\n            position_ids = position_ids.view(-1, seq_length).long()\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n        # embed positions\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n            )\n        attention_mask = self._prepare_decoder_attention_mask(\n            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n        )\n\n        hidden_states = inputs_embeds\n\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = () if use_cache else None\n\n        for idx, decoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            past_key_value = past_key_values[idx] if past_key_values is not None else None\n\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        # None for past_key_value\n                        return module(*inputs, output_attentions, None)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(decoder_layer),\n                    hidden_states,\n                    attention_mask,\n                    position_ids,\n                    None,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model.LlamaRMSNorm","title":"<code>LlamaRMSNorm</code>","text":"<p>             Bases: <code>nn.Module</code></p> Source code in <code>src/aeiva/model/macaw_model.py</code> <pre><code>class LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n\"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n\n        # convert into half-precision if necessary\n        if self.weight.dtype in [torch.float16, torch.bfloat16]:\n            hidden_states = hidden_states.to(self.weight.dtype)\n\n        return self.weight * hidden_states\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model.LlamaRMSNorm.__init__","title":"<code>__init__(hidden_size, eps=1e-06)</code>","text":"<p>LlamaRMSNorm is equivalent to T5LayerNorm</p> Source code in <code>src/aeiva/model/macaw_model.py</code> <pre><code>def __init__(self, hidden_size, eps=1e-6):\n\"\"\"\n    LlamaRMSNorm is equivalent to T5LayerNorm\n    \"\"\"\n    super().__init__()\n    self.weight = nn.Parameter(torch.ones(hidden_size))\n    self.variance_epsilon = eps\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model.MM_LLMs_Config","title":"<code>MM_LLMs_Config</code>","text":"<p>             Bases: <code>PretrainedConfig</code></p> Source code in <code>src/aeiva/model/macaw_model.py</code> <pre><code>class MM_LLMs_Config(PretrainedConfig):\n    model_type = 'mm_llms'\n    is_composition = True\n\n    def __init__(self, n_frames=6, attention_heads=8, image_conv_kernel=48, image_conv_stride=36, \n    video_conv_kernel=36, video_conv_stride=30, audio_conv_kernel=240, audio_conv_stride=220,\n    clip_config=None, whisper_config=None, llm_config=None, **kwargs):\n\n        self.image_config = clip_config\n        self.audio_config = whisper_config\n        self.llm_config = llm_config\n        self.n_frames = n_frames\n        self.attention_heads = attention_heads\n        self.image_conv_kernel = image_conv_kernel\n        self.image_conv_stride = image_conv_stride\n        self.video_conv_kernel = video_conv_kernel\n        self.video_conv_stride = video_conv_stride\n        self.audio_conv_kernel = audio_conv_kernel\n        self.audio_conv_stride = audio_conv_stride\n\n        self.hidden_size = max(llm_config.hidden_size, clip_config.projection_dim, whisper_config.d_model, clip_config.projection_dim)\n\n        super().__init__(**kwargs)\n\n    def to_dict(self):\n\"\"\"\n        Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].\n\n        Returns:\n            `Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n        \"\"\"\n        output = copy.deepcopy(self.__dict__)\n        output[\"image_config\"] = self.image_config.to_dict()\n        output[\"audio_config\"] = self.audio_config.to_dict()\n        output['llm_config'] = self.llm_config.to_dict()\n        output['n_frames'] = self.n_frames\n        output['attention_heads'] = self.attention_heads\n        output['image_conv_kernel'] = self.image_conv_kernel\n        output['image_conv_stride'] = self.image_conv_stride\n        output['video_conv_kernel'] = self.video_conv_kernel\n        output['video_conv_stride'] = self.video_conv_stride\n        output['audio_conv_kernel'] = self.audio_conv_kernel\n        output['audio_conv_stride'] = self.audio_conv_stride\n        output['hidden_size'] = self.hidden_size\n        output[\"model_type\"] = self.__class__.model_type\n        return output\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n\n        clip_config = CLIPConfig.from_dict(config_dict['image_config'])\n        whisper_config = WhisperConfig.from_dict(config_dict['audio_config'])\n        llm_config = LlamaConfig.from_dict(config_dict['llm_config'])\n\n        return cls(clip_config=clip_config, whisper_config=whisper_config, llm_config=llm_config, **kwargs)\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model.MM_LLMs_Config.to_dict","title":"<code>to_dict()</code>","text":"<p>Serializes this instance to a Python dictionary. Override the default [<code>~PretrainedConfig.to_dict</code>].</p> <p>Returns:</p> Type Description <p><code>Dict[str, any]</code>: Dictionary of all the attributes that make up this configuration instance,</p> Source code in <code>src/aeiva/model/macaw_model.py</code> <pre><code>def to_dict(self):\n\"\"\"\n    Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].\n\n    Returns:\n        `Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n    \"\"\"\n    output = copy.deepcopy(self.__dict__)\n    output[\"image_config\"] = self.image_config.to_dict()\n    output[\"audio_config\"] = self.audio_config.to_dict()\n    output['llm_config'] = self.llm_config.to_dict()\n    output['n_frames'] = self.n_frames\n    output['attention_heads'] = self.attention_heads\n    output['image_conv_kernel'] = self.image_conv_kernel\n    output['image_conv_stride'] = self.image_conv_stride\n    output['video_conv_kernel'] = self.video_conv_kernel\n    output['video_conv_stride'] = self.video_conv_stride\n    output['audio_conv_kernel'] = self.audio_conv_kernel\n    output['audio_conv_stride'] = self.audio_conv_stride\n    output['hidden_size'] = self.hidden_size\n    output[\"model_type\"] = self.__class__.model_type\n    return output\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model.WhisperEncoder","title":"<code>WhisperEncoder</code>","text":"<p>             Bases: <code>WhisperPreTrainedModel</code></p> <p>Transformer encoder consisting of config.encoder_layers self attention layers. Each layer is a [<code>WhisperEncoderLayer</code>].</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>WhisperConfig</code> <p>WhisperConfig</p> required Source code in <code>src/aeiva/model/macaw_model.py</code> <pre><code>class WhisperEncoder(WhisperPreTrainedModel):\n\"\"\"\n    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n    [`WhisperEncoderLayer`].\n\n    Args:\n        config: WhisperConfig\n    \"\"\"\n\n    def __init__(self, config: WhisperConfig):\n        super().__init__(config)\n        self.dropout = config.dropout\n        self.layerdrop = config.encoder_layerdrop\n\n        embed_dim = config.d_model\n        self.num_mel_bins = config.num_mel_bins\n        self.padding_idx = config.pad_token_id\n        self.max_source_positions = config.max_source_positions\n        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n\n        self.conv1 = nn.Conv1d(self.num_mel_bins, embed_dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=2, padding=1)\n\n        self.embed_positions = nn.Embedding(self.max_source_positions, embed_dim)\n\n        self.layers = nn.ModuleList([WhisperEncoderLayer(config) for _ in range(config.encoder_layers)])\n        self.layer_norm = nn.LayerNorm(config.d_model)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def _freeze_parameters(self):\n        for param in self.parameters():\n            param.requires_grad = False\n        self._requires_grad = False\n\n    def get_input_embeddings(self) -&gt; nn.Module:\n        return self.conv1\n\n    def set_input_embeddings(self, value: nn.Module):\n        self.conv1 = value\n\n    def forward(\n        self,\n        input_features,\n        attention_mask=None,\n        head_mask=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\nr\"\"\"\n        Args:\n            input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n                Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n                obtained by loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a\n                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n                `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n                and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n            attention_mask (`torch.Tensor`)`, *optional*):\n                Whisper does not support masking of the `input_features`, this argument is preserved for compatibility,\n                but it is not used. By default the silence in the input log mel spectrogram are ignored.\n            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n        inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n\n        inputs_embeds = inputs_embeds.permute(0, 2, 1)\n        embed_pos = self.embed_positions.weight\n\n        hidden_states = inputs_embeds + embed_pos\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\n        encoder_states = () if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n\n        # check if head_mask has a correct number of layers specified if desired\n        if head_mask is not None:\n            assert head_mask.size()[0] == (\n                len(self.layers)\n            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n\n        for idx, encoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                encoder_states = encoder_states + (hidden_states,)\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            dropout_probability = random.uniform(0, 1)\n            if self.training and (dropout_probability &lt; self.layerdrop):  # skip the layer\n                layer_outputs = (None, None)\n            else:\n                if self.gradient_checkpointing and self.training:\n\n                    def create_custom_forward(module):\n                        def custom_forward(*inputs):\n                            return module(*inputs, output_attentions)\n\n                        return custom_forward\n\n                    layer_outputs = torch.utils.checkpoint.checkpoint(\n                        create_custom_forward(encoder_layer),\n                        hidden_states,\n                        None,\n                        (head_mask[idx] if head_mask is not None else None),\n                    )\n                else:\n                    layer_outputs = encoder_layer(\n                        hidden_states,\n                        None,\n                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                        output_attentions=output_attentions,\n                    )\n\n                hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n\n        hidden_states = self.layer_norm(hidden_states)\n        if output_hidden_states:\n            encoder_states = encoder_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n        return BaseModelOutput(\n            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n        )\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model.WhisperEncoder.forward","title":"<code>forward(input_features, attention_mask=None, head_mask=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`</code> <p>Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by loading a <code>.flac</code> or <code>.wav</code> audio file into an array of type <code>List[float]</code> or a <code>numpy.ndarray</code>, e.g. via the soundfile library (<code>pip install soundfile</code>). To prepare the array into <code>input_features</code>, the [<code>AutoFeatureExtractor</code>] should be used for extracting the mel features, padding and conversion into a tensor of type <code>torch.FloatTensor</code>. See [<code>~WhisperFeatureExtractor.__call__</code>]</p> required <code>attention_mask</code> <code>`torch.Tensor`)`, *optional*</code> <p>Whisper does not support masking of the <code>input_features</code>, this argument is preserved for compatibility, but it is not used. By default the silence in the input log mel spectrogram are ignored.</p> <code>None</code> <code>head_mask</code> <code>`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*</code> <p>Mask to nullify selected heads of the attention modules. Mask values selected in <code>[0, 1]</code>:</p> <ul> <li>1 indicates the head is not masked,</li> <li>0 indicates the head is masked.</li> </ul> <code>None</code> <code>output_attentions</code> <code>`bool`, *optional*</code> <p>Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned tensors for more detail.</p> <code>None</code> <code>output_hidden_states</code> <code>`bool`, *optional*</code> <p>Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for more detail.</p> <code>None</code> <code>return_dict</code> <code>`bool`, *optional*</code> <p>Whether or not to return a [<code>~utils.ModelOutput</code>] instead of a plain tuple.</p> <code>None</code> Source code in <code>src/aeiva/model/macaw_model.py</code> <pre><code>def forward(\n    self,\n    input_features,\n    attention_mask=None,\n    head_mask=None,\n    output_attentions=None,\n    output_hidden_states=None,\n    return_dict=None,\n):\nr\"\"\"\n    Args:\n        input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n            Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n            obtained by loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a\n            `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n            `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n            and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n        attention_mask (`torch.Tensor`)`, *optional*):\n            Whisper does not support masking of the `input_features`, this argument is preserved for compatibility,\n            but it is not used. By default the silence in the input log mel spectrogram are ignored.\n        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n            Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n            returned tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n            for more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n    \"\"\"\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n    inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n    inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n\n    inputs_embeds = inputs_embeds.permute(0, 2, 1)\n    embed_pos = self.embed_positions.weight\n\n    hidden_states = inputs_embeds + embed_pos\n    hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\n    encoder_states = () if output_hidden_states else None\n    all_attentions = () if output_attentions else None\n\n    # check if head_mask has a correct number of layers specified if desired\n    if head_mask is not None:\n        assert head_mask.size()[0] == (\n            len(self.layers)\n        ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n\n    for idx, encoder_layer in enumerate(self.layers):\n        if output_hidden_states:\n            encoder_states = encoder_states + (hidden_states,)\n        # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n        dropout_probability = random.uniform(0, 1)\n        if self.training and (dropout_probability &lt; self.layerdrop):  # skip the layer\n            layer_outputs = (None, None)\n        else:\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs, output_attentions)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(encoder_layer),\n                    hidden_states,\n                    None,\n                    (head_mask[idx] if head_mask is not None else None),\n                )\n            else:\n                layer_outputs = encoder_layer(\n                    hidden_states,\n                    None,\n                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                    output_attentions=output_attentions,\n                )\n\n            hidden_states = layer_outputs[0]\n\n        if output_attentions:\n            all_attentions = all_attentions + (layer_outputs[1],)\n\n    hidden_states = self.layer_norm(hidden_states)\n    if output_hidden_states:\n        encoder_states = encoder_states + (hidden_states,)\n\n    if not return_dict:\n        return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n    return BaseModelOutput(\n        last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n    )\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model.rotate_half","title":"<code>rotate_half(x)</code>","text":"<p>Rotates half the hidden dims of the input.</p> Source code in <code>src/aeiva/model/macaw_model.py</code> <pre><code>def rotate_half(x):\n\"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model_old","title":"<code>macaw_model_old</code>","text":"<p>This script contains the implementation of the MACAW model. MACAW is a multimodal transformer model that combines the CLIP and Whisper models.</p> <p>Author: Bang Liu Date: 2023-06-22</p> <p>References: - Macaw-LLM code repository: https://github.com/lyuchenyang/Macaw-LLM/blob/main/modeling.py</p>"},{"location":"reference/#src.aeiva.model.macaw_model_old.LlamaAttention","title":"<code>LlamaAttention</code>","text":"<p>             Bases: <code>nn.Module</code></p> <p>Multi-headed attention from 'Attention Is All You Need' paper</p> Source code in <code>src/aeiva/model/macaw_model_old.py</code> <pre><code>class LlamaAttention(nn.Module):\n\"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.max_position_embeddings = config.max_position_embeddings  # !!! I want to change this variable name.\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n        self.rotary_emb = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n    ) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        # By placing the num_heads dimension as the second dimension, it allows for \n        # efficient batched matrix operations (e.g., matrix multiplication in attention computation) \n        # across all the heads. It is basically a data layout optimization for computational efficiency \n        # in the context of multi-head attention.\n        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]  # the shape is [batch_size, num_heads, seq_len, head_dim], so -2 dimension is 'seq_len'\n        if past_key_value is not None:  \n            # If past_key_value is not None, this means the model is being used in an autoregressive setting, \n            # where the past key-value pairs are given to the current step.\n            # past_key_value[0] refers to the previously computed key states,\n            # past_key_value[1] refers to the previously computed value states.\n            # The shape of past_key_value[0] and past_key_value[1] is [batch_size, num_heads, seq_len, head_dim].\n            kv_seq_len += past_key_value[0].shape[-2]  # + past seq_len\n\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            # This following line is ensuring numerical stability. It caps the minimum value of the attention weights\n            # to be the minimum finite representable number for the data type of attn_weights. This avoids \n            # potential issues with underflow when these weights are later passed through the softmax function.\n            attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))\n\n        # upcast attention to fp32\n        # This is done to prevent numerical instability that can occur\n        # during operations on very small numbers or very large numbers.\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size) # self.hidden_size is equivalent to self.num_heads * self.head_dim\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model_old.LlamaDecoderLayer","title":"<code>LlamaDecoderLayer</code>","text":"<p>             Bases: <code>nn.Module</code></p> Source code in <code>src/aeiva/model/macaw_model_old.py</code> <pre><code>class LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.self_attn = LlamaAttention(config=config)\n        self.mlp = LlamaMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n        )\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n    ) -&gt; Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n\"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model_old.LlamaDecoderLayer.forward","title":"<code>forward(hidden_states, attention_mask=None, position_ids=None, past_key_value=None, output_attentions=False, use_cache=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>hidden_states</code> <code>`torch.FloatTensor`</code> <p>input to the layer of shape <code>(batch, seq_len, embed_dim)</code></p> required <code>attention_mask</code> <code>`torch.FloatTensor`, *optional*</code> <p>attention mask of size <code>(batch, 1, tgt_len, src_len)</code> where padding elements are indicated by very large negative values.</p> <code>None</code> <code>output_attentions</code> <code>`bool`, *optional*</code> <p>Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned tensors for more detail.</p> <code>False</code> <code>use_cache</code> <code>`bool`, *optional*</code> <p>If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see <code>past_key_values</code>).</p> <code>False</code> <code>past_key_value</code> <code>`Tuple(torch.FloatTensor)`, *optional*</code> <p>cached past key and value projection states</p> <code>None</code> Source code in <code>src/aeiva/model/macaw_model_old.py</code> <pre><code>def forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: Optional[bool] = False,\n    use_cache: Optional[bool] = False,\n) -&gt; Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n\"\"\"\n    Args:\n        hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n        attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n            `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n            returned tensors for more detail.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n            (see `past_key_values`).\n        past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n    \"\"\"\n\n    residual = hidden_states\n\n    hidden_states = self.input_layernorm(hidden_states)\n\n    # Self Attention\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n        hidden_states=hidden_states,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        past_key_value=past_key_value,\n        output_attentions=output_attentions,\n        use_cache=use_cache,\n    )\n    hidden_states = residual + hidden_states\n\n    # Fully Connected\n    residual = hidden_states\n    hidden_states = self.post_attention_layernorm(hidden_states)\n    hidden_states = self.mlp(hidden_states)\n    hidden_states = residual + hidden_states\n\n    outputs = (hidden_states,)\n\n    if output_attentions:\n        outputs += (self_attn_weights,)\n\n    if use_cache:\n        outputs += (present_key_value,)\n\n    return outputs\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model_old.LlamaModel","title":"<code>LlamaModel</code>","text":"<p>             Bases: <code>LlamaPreTrainedModel</code></p> <p>Transformer decoder consisting of config.num_hidden_layers layers. Each layer is a [<code>LlamaDecoderLayer</code>]</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LlamaConfig</code> <p>LlamaConfig</p> required Source code in <code>src/aeiva/model/macaw_model_old.py</code> <pre><code>class LlamaModel(LlamaPreTrainedModel):\n\"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        # embedding layer, stacked decoder layers, and layer normalization in llama.\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n\n        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n        # Gradient checkpointing is a technique to reduce the memory usage when training deep neural networks.\n        # In deep learning, when you perform backpropagation to compute gradients and update the model parameters,\n        # you need to store the intermediate activations from the forward pass, so you can use them in the backward pass. \n        # For large models or long sequences, this can consume a lot of memory.\n        # \n        # Gradient checkpointing addresses this by not storing all the intermediate activations in memory during the forward pass. \n        # Instead, it stores only a subset of the activations, and recomputes the rest during the backward pass as needed. \n        # This trades off computation time (because you need to recompute some values) for memory usage.\n        # \n        # This technique is particularly useful when training large models that would otherwise not fit into GPU memory. \n        # However, it can slow down training because of the extra computation.\n        self.gradient_checkpointing = False\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n        # create causal mask\n        # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]\n        combined_attention_mask = None\n        if input_shape[-1] &gt; 1:  # seq_len &gt; 1\n            combined_attention_mask = _make_causal_mask(\n                input_shape,\n                inputs_embeds.dtype,\n                device=inputs_embeds.device,\n                past_key_values_length=past_key_values_length,\n            )\n\n        if attention_mask is not None:\n            # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]\n            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n                inputs_embeds.device\n            )\n            combined_attention_mask = (\n                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n            )\n\n        return combined_attention_mask\n\n    # @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -&gt; Union[Tuple, BaseModelOutputWithPast]:\n        # set output and cache flags\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # prepare input_ids/inputs_embeds\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n        elif input_ids is not None:\n            batch_size, seq_length = input_ids.shape\n        elif inputs_embeds is not None:\n            batch_size, seq_length, _ = inputs_embeds.shape\n        else:\n            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        # prepare attention mask and other parameters for decoder layers\n        past_key_values_length = 0\n        seq_length_with_past = seq_length\n\n        if past_key_values is not None:\n            past_key_values_length = past_key_values[0][0].shape[2]\n            seq_length_with_past = seq_length_with_past + past_key_values_length\n\n        if position_ids is None:\n            device = input_ids.device if input_ids is not None else inputs_embeds.device\n            position_ids = torch.arange(\n                past_key_values_length, past_key_values_length + seq_length, dtype=torch.long, device=device\n            )\n            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n        else:\n            position_ids = position_ids.view(-1, seq_length).long()\n\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n            )\n        attention_mask = self._prepare_decoder_attention_mask(\n            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n        )\n\n        hidden_states = inputs_embeds\n\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n        # forward through all decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = () if use_cache else None\n\n        for idx, decoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            past_key_value = past_key_values[idx] if past_key_values is not None else None\n\n            if self.gradient_checkpointing and self.training:\n                # define the function for gradient checkpointing\n                # in checkpointing, we need to create a custom function for the forward pass \n                # (the custom_forward function in your code) and then using the \n                # torch.utils.checkpoint.checkpoint function to apply this custom function \n                # with gradient checkpointing.\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs, output_attentions, None)  # None for past_key_value\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(decoder_layer),\n                    hidden_states,\n                    attention_mask,\n                    position_ids,\n                    None,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n\n        # output the hidden states, the self attentions and the cache (if needed)\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model_old.LlamaRMSNorm","title":"<code>LlamaRMSNorm</code>","text":"<p>             Bases: <code>nn.Module</code></p> Source code in <code>src/aeiva/model/macaw_model_old.py</code> <pre><code>class LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n\"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        The overall effect of this layer is to ensure that,\n        for each feature in the hidden_states,\n        the activations have zero mean and unit variance across the batch.\n        This can make the training process more stable and faster.\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))  # trainable parameter for affine transformation\n        self.variance_epsilon = eps  # for numerical stability\n\n    def forward(self, hidden_states):\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n\n        # convert into half-precision if necessary\n        if self.weight.dtype in [torch.float16, torch.bfloat16]:\n            hidden_states = hidden_states.to(self.weight.dtype)\n\n        return self.weight * hidden_states\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model_old.LlamaRMSNorm.__init__","title":"<code>__init__(hidden_size, eps=1e-06)</code>","text":"<p>LlamaRMSNorm is equivalent to T5LayerNorm The overall effect of this layer is to ensure that, for each feature in the hidden_states, the activations have zero mean and unit variance across the batch. This can make the training process more stable and faster.</p> Source code in <code>src/aeiva/model/macaw_model_old.py</code> <pre><code>def __init__(self, hidden_size, eps=1e-6):\n\"\"\"\n    LlamaRMSNorm is equivalent to T5LayerNorm\n    The overall effect of this layer is to ensure that,\n    for each feature in the hidden_states,\n    the activations have zero mean and unit variance across the batch.\n    This can make the training process more stable and faster.\n    \"\"\"\n    super().__init__()\n    self.weight = nn.Parameter(torch.ones(hidden_size))  # trainable parameter for affine transformation\n    self.variance_epsilon = eps  # for numerical stability\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model_old.LlamaRotaryEmbedding","title":"<code>LlamaRotaryEmbedding</code>","text":"<p>             Bases: <code>torch.nn.Module</code></p> <p>Rotary embedding described in: https://arxiv.org/pdf/2104.09864.pdf. It is used to modulate the position information in the input embeddings. Llama used rotary embedding.</p> Source code in <code>src/aeiva/model/macaw_model_old.py</code> <pre><code>class LlamaRotaryEmbedding(torch.nn.Module):\n\"\"\"\n    Rotary embedding described in: https://arxiv.org/pdf/2104.09864.pdf.\n    It is used to modulate the position information in the input embeddings.\n    Llama used rotary embedding.\n    \"\"\"\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n        # Compute the inverse frequencies, which will be used to modulate the position information\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n        # The register_buffer() function is used in PyTorch to register a tensor that is not a parameter,\n        # but you still want it to be a part of the model's state. It's used for tensors that should\n        # have their state saved in the model's state_dict and should be moved to the device with the rest of the model.\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        # Build here to make `torch.jit.trace` work.\n        # max_position_embeddings: max sequence length that this model might ever be used with\n        self.max_seq_len_cached = max_position_embeddings\n\n        # Compute the positional encodings (both cos and sin parts)\n        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n        freqs = torch.einsum(\"i,j-&gt;ij\", t, self.inv_freq)\n\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n\n    def forward(self, x, seq_len=None):\n        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.\n        # x.shape: [batch_size, num_attention_heads, sequence_length, head_size].\n        # The forward function then outputs two tensors, each of which is a sin or cos embedding representation of the input x. \n        # Both output tensors will have a shape of [1, 1, sequence_length, head_size].\n        # NOTE: Only the dtype and device attributes of x are relevant here. The values are not used.\n        if seq_len &gt; self.max_seq_len_cached:\n            self.max_seq_len_cached = seq_len\n            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)\n            freqs = torch.einsum(\"i,j-&gt;ij\", t, self.inv_freq)\n            # Different from paper, but it uses a different permutation in order to obtain the same calculation\n            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n            self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n            self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n        return (\n            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n        )\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model_old.MM_LLMs","title":"<code>MM_LLMs</code>","text":"<p>             Bases: <code>PreTrainedModel</code></p> <p>This is the multimodal language model that combines CLIP and Whisper encoders with a language model. We need a config file to specify the multimodal encoder configurations.</p> Source code in <code>src/aeiva/model/macaw_model_old.py</code> <pre><code>class MM_LLMs(PreTrainedModel):\n\"\"\"\n    This is the multimodal language model that combines CLIP and Whisper encoders with a language model.\n    We need a config file to specify the multimodal encoder configurations.\n    \"\"\"\n    def __init__(self, config):\n        super().__init__(config)\n        # multimodal config\n        self.config = config\n\n        # multimodal encoders\n        self.image_encoder = CLIPModel(config.image_config)  # NOTE: here they use CLIP for both image and video.\n        self.video_encoder = CLIPModel(config.image_config)\n        self.audio_encoder = WhisperModel(config.audio_config)\n        self.llm = LlamaForCausalLM(config.llm_config)\n\n        # video temporal position embedding layer\n        self.temporal_position_embeddings = nn.Embedding(\n            config.n_frames, \n            config.image_config.projection_dim)\n\n        # multimodal attention layers for mapping multimodal features to the same space\n        attn_dropout = 0.1\n        is_add_bias_kv = True\n        is_add_zero_attn = True\n        self.temporal_self_attention = nn.MultiheadAttention(config.image_config.projection_dim,\n                                                             config.attention_heads,\n                                                             dropout=attn_dropout,\n                                                             add_bias_kv=is_add_bias_kv,\n                                                             add_zero_attn=is_add_zero_attn)\n        self.video_align_attention = nn.MultiheadAttention(config.llm_config.hidden_size, \n                                                             config.attention_heads,\n                                                             dropout=attn_dropout,\n                                                             add_bias_kv=is_add_bias_kv,\n                                                             add_zero_attn=is_add_zero_attn)\n        self.audio_align_attention = nn.MultiheadAttention(config.llm_config.hidden_size, \n                                                             config.attention_heads,\n                                                             dropout=attn_dropout,\n                                                             add_bias_kv=is_add_bias_kv,\n                                                             add_zero_attn=is_add_zero_attn)\n        self.image_align_attention = nn.MultiheadAttention(config.llm_config.hidden_size, \n                                                             config.attention_heads,\n                                                             dropout=attn_dropout,\n                                                             add_bias_kv=is_add_bias_kv,\n                                                             add_zero_attn=is_add_zero_attn)\n\n        # multimodal projection layers for mapping multimodal features to the same space\n        self.transform_video_to_hidden = nn.Linear(config.image_config.projection_dim, \n                                                   config.llm_config.hidden_size)\n        self.transform_audio_to_hidden = nn.Linear(config.audio_config.d_model, \n                                                   config.llm_config.hidden_size)\n        self.transform_image_to_hidden = nn.Linear(config.image_config.projection_dim, \n                                                   config.llm_config.hidden_size)\n\n        self.project_image = nn.Conv1d(config.image_config.projection_dim, config.image_config.projection_dim, \n        kernel_size=48, stride=36)\n        self.project_video = nn.Conv1d(config.image_config.projection_dim, config.image_config.projection_dim, \n        kernel_size=36, stride=30)\n        self.project_audio = nn.Conv1d(config.audio_config.d_model, config.audio_config.d_model, \n        kernel_size=240, stride=220)\n\n        # multimodal fusion layers\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n\n        self.layer_norm = nn.LayerNorm(config.image_config.projection_dim)\n        self.softmax = nn.Softmax(dim=-1)\n        self.relu = nn.ReLU()\n        self.gelu = nn.GELU()\n        self.elu = nn.ELU()\n        self.sigmoid = nn.Sigmoid()\n\n        self.loss_fct = CrossEntropyLoss()\n\n        self.init_weights()\n\n    def forward(self, inputs=None):\n        # \"\"\"\n        # :param inputs:\n        #             video_frames: (B x F)\n        #             audios: B x 1\n        #             images: B x 1\n        #             input_ids: B x L\n        #             labels: B x L\n        #\n        # :return: the output of the language model LlamaForCausalLM.\n        # \"\"\"\n        text_embeddings, attention_mask, labels = self.prepare_inputs_for_generation(inputs)\n\n        if 'inference' in inputs and inputs['inference'] is True:\n            # generate_ids = self.llm.generate(input_ids=inputs['input_ids'], inputs_embeds=text_embeddings, max_new_tokens=128)\n            # generate_ids = self.llm.generate(inputs_embeds=text_embeddings, max_new_tokens=128)\n\n            # !!! The code below will possibly trigger an error in : https://github.com/microsoft/DeepSpeed/issues/3156 (the solution only partially resolves the bug for me)\n            generate_ids = self.llm.generate(\n                inputs_embeds=text_embeddings, max_new_tokens=128, eos_token_id=2, bos_token_id=1, pad_token_id=32006  # !!! revise later. use config constants instead.\n                )\n            return generate_ids\n        outputs = self.llm(inputs_embeds=text_embeddings, attention_mask=attention_mask, labels=labels)\n\n        return outputs\n\n    def prepare_inputs_for_generation(self, inputs):\n\"\"\"\n        The purpose of this method is to integrate the different modalities into the text embeddings \n        and prepare the associated attention mask and labels for the language model, so the model can \n        generate text conditioned on all the input modalities.\n\n        inputs is a dictionary containing the following keys: (!!! my hypothesis)\n            video_frames: (B x F)\n            audios: B x 1\n            images: B x 1\n            input_ids: B x L\n            attention_mask: B x L\n            labels: B x L\n            video_starts: B x 1\n            video_ends: B x 1\n            audio_starts: B x 1\n            audio_ends: B x 1\n            image_starts: B x 1\n            image_ends: B x 1\n            inference: True/False\n        \"\"\"\n        # get multimodal embeddings\n        image_features = self.encode_image(inputs['images']) if inputs['images'] is not None else None\n        audio_features = self.encode_audio(inputs['audios']) if inputs['audios'] is not None else None\n        video_features = self.encode_video(inputs['videos']) if inputs['videos'] is not None else None\n        embed_tokens = self.llm.model.embed_tokens\n\n\n        # for debug !!!!!!\n        # Find maximum id in input_ids\n        max_id = torch.max(inputs['input_ids'])\n        print(f\"Max ID in input_ids: {max_id.item()}\")\n\n        # Get vocab size from embedding layer\n        vocab_size = embed_tokens.num_embeddings\n        print(f\"Vocabulary size: {vocab_size}\")\n\n\n\n        text_embeddings = embed_tokens(inputs['input_ids'])\n\n        token_embeddings = embed_tokens.weight.unsqueeze(0).repeat(\n            text_embeddings.size(0), 1, 1).transpose(0, 1)\n\n        # ignore_num seems to be a counter that tracks the total size (or length) of the \n        # multimodal input segments (video, audio, image) added to the original text inputs.\n        ingore_num = 0\n\n        # project and merge video features to the same space as text embeddings\n        if video_features is not None:\n            # get video starts and ends embeddings\n            video_starts = embed_tokens(inputs['video_starts']).unsqueeze(1)\n            video_ends = embed_tokens(inputs['video_ends']).unsqueeze(1)\n\n            # project video features to the same space as text embeddings\n            video_features = self.transform_video_to_hidden(video_features)\n\n            video_features = self.video_align_attention(\n                video_features.transpose(0, 1), token_embeddings, token_embeddings)[0].transpose(0, 1).contiguous()\n\n            # concatenate video starts, video features, and video ends embeddings\n            video_inputs = torch.cat([torch.cat([video_starts, video_features], dim=1), video_ends], dim=1)\n\n            # concatenate video inputs to the original text embeddings\n            # NOTE: the first token of text_embeddings keeps at the same position\n            text_embeddings = torch.cat([torch.cat([text_embeddings[:, 0, :].unsqueeze(1), video_inputs], dim=1), text_embeddings[:, 1:, :]], dim=1)\n\n            ingore_num += (video_inputs.size(1))\n\n        # project and merge audio features to the same space as text embeddings\n        if audio_features is not None:\n            # get audio starts and ends embeddings\n            audio_starts = embed_tokens(inputs['audio_starts']).unsqueeze(1)\n            audio_ends = embed_tokens(inputs['audio_ends']).unsqueeze(1)\n\n            # project audio features to the same space as text embeddings\n            audio_features = self.project_audio(audio_features.transpose(1, 2).contiguous()).transpose(1, 2).contiguous()\n            audio_features = self.transform_audio_to_hidden(audio_features)\n            # mean pooling\n            # audio_features = torch.sum(audio_features, dim=1) / audio_features.size(1) \n            # audio_features = audio_features.unsqueeze(1)\n            audio_features = self.audio_align_attention(\n                audio_features.transpose(0, 1), token_embeddings, token_embeddings)[0].transpose(0, 1).contiguous()\n\n            # concatenate audio starts, audio features, and audio ends embeddings\n            audio_inputs = torch.cat([torch.cat([audio_starts, audio_features], dim=1), audio_ends], dim=1)\n\n            # concatenate audio inputs to the original text embeddings\n            text_embeddings = torch.cat(\n                [torch.cat([text_embeddings[:, 0, :].unsqueeze(1), audio_inputs], dim=1), text_embeddings[:, 1:, :]],\n                dim=1)\n\n            ingore_num += (audio_inputs.size(1))\n\n        # project and merge image features to the same space as text embeddings\n        if image_features is not None:\n            # get image starts and ends embeddings\n            image_starts = embed_tokens(inputs['image_starts']).unsqueeze(1)\n            image_ends = embed_tokens(inputs['image_ends']).unsqueeze(1)\n\n            # project image features to the same space as text embeddings\n            image_features = self.project_image(image_features.transpose(1, 2).contiguous()).transpose(1, 2).contiguous()\n            image_features = self.transform_image_to_hidden(image_features)\n            image_features = self.image_align_attention(\n                image_features.transpose(0, 1), token_embeddings, token_embeddings)[0].transpose(0, 1).contiguous()\n\n            # concatenate image starts, image features, and image ends embeddings\n            image_inputs = torch.cat([torch.cat([image_starts, image_features], dim=1), image_ends], dim=1)\n\n            # concatenate image inputs to the original text embeddings\n            text_embeddings = torch.cat(\n                [torch.cat([text_embeddings[:, 0, :].unsqueeze(1), image_inputs], dim=1), \n                text_embeddings[:, 1:, :]], dim=1)\n\n            ingore_num += (image_inputs.size(1))\n\n        if 'attention_mask' in inputs:\n            # increase the length of attention mask by adding the length of multimodal inputs\n            attention_mask = torch.tensor([1]*ingore_num*text_embeddings.size(0), device=text_embeddings.device).view(text_embeddings.size(0), -1)  # (B X ignore_num)\n            attention_mask = torch.cat([attention_mask, inputs['attention_mask']], dim=1)\n        else:\n            attention_mask = None\n\n        if 'labels' in inputs and inputs['labels'] is not None:\n            # increase the length of labels by adding the length of labels\n            # we use -100 to ignore the loss of labels in multimodal inputs\n            # !!! we can replace -100 by config constants to make the code better\n\n            # since the tokens corresponding to the image_inputs, audio_inputs, and video_inputs are not part of the original text \n            # and don't have corresponding labels in the true text sequence, their labels are set to -100. This ensures that \n            # the model's predictions for these tokens don't affect the loss and, consequently, the gradients and the model's subsequent learning.\n            labels = torch.tensor([-100]*ingore_num*text_embeddings.size(0), device=text_embeddings.device).view(text_embeddings.size(0), -1)\n            labels = torch.cat([labels, inputs['labels']], dim=1)\n        else:\n            labels = None\n\n        # text_embeddings: (batch_size, sequence_length + ingore_num, embedding_dim)\n        # attention_mask: (batch_size, sequence_length + ingore_num). 1 denotes we should attend to, and 0 denotes we should not attend to the token.\n        # labels: (batch_size, sequence_length + ingore_num). -100 denotes we should ignore the token.\n        return text_embeddings, attention_mask, labels\n\n    def encode_video(self, videos):\n\"\"\"\n        Encode video features to video embeddings.\n\n        Args:\n            videos: (batch_size, n_frames, n_channels, height, width)\n\n        Returns:\n            video_embeddings: (batch_size, n_frames, embedding_dim)\n        \"\"\"\n        # simple image encoding without temporal embedding and self attention\n        # Reference: https://huggingface.co/docs/transformers/model_doc/clip\n        videos = videos.view(-1, videos.size(-3), videos.size(-2), videos.size(-1))  # pixel_values (torch.FloatTensor of shape (batch_size * n_frames, num_channels, height, width)) \n        video_outputs = self.video_encoder.get_image_features(videos)  # image_features (torch.FloatTensor of shape (batch_size * n_frames, output_dim)\n        video_features = video_outputs\n        temporal_pos = torch.tensor(\n            [[i for i in range(self.config.n_frames)] \n            for j in range(videos.size(0) // self.config.n_frames)],\n            dtype=torch.int, device=video_features.device).view(-1)  # 2d indices to 1d indices, shape: (batch_size * n_frames)\n\n        frame_temporal_pos_embed = self.temporal_position_embeddings(temporal_pos)\n\n        video_features = (video_features + frame_temporal_pos_embed).view(\n            videos.size(0) // self.config.n_frames, self.config.n_frames, -1)  # (batch_size, n_frames, output_dim)\n\n        video_features = video_features.transpose(0, 1).contiguous()\n        # nn.MultiheadAttention takes query, key, value as inputs. Their shapes are (sequence_length, batch_size, embedding_dim).\n        # The outputs are two elements: attn_output of shape (sequence_length, batch_size, embedding_dim), and attn_output_weights of shape (batch_size, sequence_length, sequence_length).\n        self_attn_video_features = self.temporal_self_attention(video_features, video_features, video_features)[0]\n\n        return self_attn_video_features.transpose(0, 1).contiguous() # (batch_size, n_frames, output_dim)\n\n    def encode_video_long(self, videos):\n\"\"\"\n        Encode video features to video embeddings.\n\n        Args:\n            videos: (batch_size, n_frames, n_channels, height, width)\n\n        Returns:\n            video_embeddings: (batch_size, n_frames, embedding_dim)\n        \"\"\"\n        # simple image encoding without temporal embedding and self attention\n        videos = videos.view(-1, videos.size(-3), videos.size(-2), videos.size(-1))  # pixel_values (torch.FloatTensor of shape (batch_size * n_frames, num_channels, height, width))\n        video_features = self.video_encoder.visual_projection(self.video_encoder.vision_model(videos)[0])[:, 1:, :]\n        video_features = video_features.reshape(\n            videos.size(0) // self.config.n_frames,\n            self.config.n_frames * video_features.size(1),\n            -1).contiguous()\n\n        return video_features\n\n    def encode_audio(self, audios):\n        audio_features = self.audio_encoder.encoder(audios)\n        return audio_features[0]\n\n    def encode_image(self, images):\n        # vision_outputs = self.image_encoder.get_image_features(images)\n        # image_features = vision_outputs  # pooled_output\n        # image_features = self.visual_projection(pooled_output)\n        # image_features = image_features.unsqueeze(1)\n        image_features = self.image_encoder.visual_projection(self.image_encoder.vision_model(images)[0])[:, 1:, :]\n        return image_features\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model_old.MM_LLMs.encode_video","title":"<code>encode_video(videos)</code>","text":"<p>Encode video features to video embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>videos</code> <p>(batch_size, n_frames, n_channels, height, width)</p> required <p>Returns:</p> Name Type Description <code>video_embeddings</code> <p>(batch_size, n_frames, embedding_dim)</p> Source code in <code>src/aeiva/model/macaw_model_old.py</code> <pre><code>def encode_video(self, videos):\n\"\"\"\n    Encode video features to video embeddings.\n\n    Args:\n        videos: (batch_size, n_frames, n_channels, height, width)\n\n    Returns:\n        video_embeddings: (batch_size, n_frames, embedding_dim)\n    \"\"\"\n    # simple image encoding without temporal embedding and self attention\n    # Reference: https://huggingface.co/docs/transformers/model_doc/clip\n    videos = videos.view(-1, videos.size(-3), videos.size(-2), videos.size(-1))  # pixel_values (torch.FloatTensor of shape (batch_size * n_frames, num_channels, height, width)) \n    video_outputs = self.video_encoder.get_image_features(videos)  # image_features (torch.FloatTensor of shape (batch_size * n_frames, output_dim)\n    video_features = video_outputs\n    temporal_pos = torch.tensor(\n        [[i for i in range(self.config.n_frames)] \n        for j in range(videos.size(0) // self.config.n_frames)],\n        dtype=torch.int, device=video_features.device).view(-1)  # 2d indices to 1d indices, shape: (batch_size * n_frames)\n\n    frame_temporal_pos_embed = self.temporal_position_embeddings(temporal_pos)\n\n    video_features = (video_features + frame_temporal_pos_embed).view(\n        videos.size(0) // self.config.n_frames, self.config.n_frames, -1)  # (batch_size, n_frames, output_dim)\n\n    video_features = video_features.transpose(0, 1).contiguous()\n    # nn.MultiheadAttention takes query, key, value as inputs. Their shapes are (sequence_length, batch_size, embedding_dim).\n    # The outputs are two elements: attn_output of shape (sequence_length, batch_size, embedding_dim), and attn_output_weights of shape (batch_size, sequence_length, sequence_length).\n    self_attn_video_features = self.temporal_self_attention(video_features, video_features, video_features)[0]\n\n    return self_attn_video_features.transpose(0, 1).contiguous() # (batch_size, n_frames, output_dim)\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model_old.MM_LLMs.encode_video_long","title":"<code>encode_video_long(videos)</code>","text":"<p>Encode video features to video embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>videos</code> <p>(batch_size, n_frames, n_channels, height, width)</p> required <p>Returns:</p> Name Type Description <code>video_embeddings</code> <p>(batch_size, n_frames, embedding_dim)</p> Source code in <code>src/aeiva/model/macaw_model_old.py</code> <pre><code>def encode_video_long(self, videos):\n\"\"\"\n    Encode video features to video embeddings.\n\n    Args:\n        videos: (batch_size, n_frames, n_channels, height, width)\n\n    Returns:\n        video_embeddings: (batch_size, n_frames, embedding_dim)\n    \"\"\"\n    # simple image encoding without temporal embedding and self attention\n    videos = videos.view(-1, videos.size(-3), videos.size(-2), videos.size(-1))  # pixel_values (torch.FloatTensor of shape (batch_size * n_frames, num_channels, height, width))\n    video_features = self.video_encoder.visual_projection(self.video_encoder.vision_model(videos)[0])[:, 1:, :]\n    video_features = video_features.reshape(\n        videos.size(0) // self.config.n_frames,\n        self.config.n_frames * video_features.size(1),\n        -1).contiguous()\n\n    return video_features\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model_old.MM_LLMs.prepare_inputs_for_generation","title":"<code>prepare_inputs_for_generation(inputs)</code>","text":"<p>The purpose of this method is to integrate the different modalities into the text embeddings  and prepare the associated attention mask and labels for the language model, so the model can  generate text conditioned on all the input modalities.</p> (!!! my hypothesis) <p>video_frames: (B x F) audios: B x 1 images: B x 1 input_ids: B x L attention_mask: B x L labels: B x L video_starts: B x 1 video_ends: B x 1 audio_starts: B x 1 audio_ends: B x 1 image_starts: B x 1 image_ends: B x 1 inference: True/False</p> Source code in <code>src/aeiva/model/macaw_model_old.py</code> <pre><code>def prepare_inputs_for_generation(self, inputs):\n\"\"\"\n    The purpose of this method is to integrate the different modalities into the text embeddings \n    and prepare the associated attention mask and labels for the language model, so the model can \n    generate text conditioned on all the input modalities.\n\n    inputs is a dictionary containing the following keys: (!!! my hypothesis)\n        video_frames: (B x F)\n        audios: B x 1\n        images: B x 1\n        input_ids: B x L\n        attention_mask: B x L\n        labels: B x L\n        video_starts: B x 1\n        video_ends: B x 1\n        audio_starts: B x 1\n        audio_ends: B x 1\n        image_starts: B x 1\n        image_ends: B x 1\n        inference: True/False\n    \"\"\"\n    # get multimodal embeddings\n    image_features = self.encode_image(inputs['images']) if inputs['images'] is not None else None\n    audio_features = self.encode_audio(inputs['audios']) if inputs['audios'] is not None else None\n    video_features = self.encode_video(inputs['videos']) if inputs['videos'] is not None else None\n    embed_tokens = self.llm.model.embed_tokens\n\n\n    # for debug !!!!!!\n    # Find maximum id in input_ids\n    max_id = torch.max(inputs['input_ids'])\n    print(f\"Max ID in input_ids: {max_id.item()}\")\n\n    # Get vocab size from embedding layer\n    vocab_size = embed_tokens.num_embeddings\n    print(f\"Vocabulary size: {vocab_size}\")\n\n\n\n    text_embeddings = embed_tokens(inputs['input_ids'])\n\n    token_embeddings = embed_tokens.weight.unsqueeze(0).repeat(\n        text_embeddings.size(0), 1, 1).transpose(0, 1)\n\n    # ignore_num seems to be a counter that tracks the total size (or length) of the \n    # multimodal input segments (video, audio, image) added to the original text inputs.\n    ingore_num = 0\n\n    # project and merge video features to the same space as text embeddings\n    if video_features is not None:\n        # get video starts and ends embeddings\n        video_starts = embed_tokens(inputs['video_starts']).unsqueeze(1)\n        video_ends = embed_tokens(inputs['video_ends']).unsqueeze(1)\n\n        # project video features to the same space as text embeddings\n        video_features = self.transform_video_to_hidden(video_features)\n\n        video_features = self.video_align_attention(\n            video_features.transpose(0, 1), token_embeddings, token_embeddings)[0].transpose(0, 1).contiguous()\n\n        # concatenate video starts, video features, and video ends embeddings\n        video_inputs = torch.cat([torch.cat([video_starts, video_features], dim=1), video_ends], dim=1)\n\n        # concatenate video inputs to the original text embeddings\n        # NOTE: the first token of text_embeddings keeps at the same position\n        text_embeddings = torch.cat([torch.cat([text_embeddings[:, 0, :].unsqueeze(1), video_inputs], dim=1), text_embeddings[:, 1:, :]], dim=1)\n\n        ingore_num += (video_inputs.size(1))\n\n    # project and merge audio features to the same space as text embeddings\n    if audio_features is not None:\n        # get audio starts and ends embeddings\n        audio_starts = embed_tokens(inputs['audio_starts']).unsqueeze(1)\n        audio_ends = embed_tokens(inputs['audio_ends']).unsqueeze(1)\n\n        # project audio features to the same space as text embeddings\n        audio_features = self.project_audio(audio_features.transpose(1, 2).contiguous()).transpose(1, 2).contiguous()\n        audio_features = self.transform_audio_to_hidden(audio_features)\n        # mean pooling\n        # audio_features = torch.sum(audio_features, dim=1) / audio_features.size(1) \n        # audio_features = audio_features.unsqueeze(1)\n        audio_features = self.audio_align_attention(\n            audio_features.transpose(0, 1), token_embeddings, token_embeddings)[0].transpose(0, 1).contiguous()\n\n        # concatenate audio starts, audio features, and audio ends embeddings\n        audio_inputs = torch.cat([torch.cat([audio_starts, audio_features], dim=1), audio_ends], dim=1)\n\n        # concatenate audio inputs to the original text embeddings\n        text_embeddings = torch.cat(\n            [torch.cat([text_embeddings[:, 0, :].unsqueeze(1), audio_inputs], dim=1), text_embeddings[:, 1:, :]],\n            dim=1)\n\n        ingore_num += (audio_inputs.size(1))\n\n    # project and merge image features to the same space as text embeddings\n    if image_features is not None:\n        # get image starts and ends embeddings\n        image_starts = embed_tokens(inputs['image_starts']).unsqueeze(1)\n        image_ends = embed_tokens(inputs['image_ends']).unsqueeze(1)\n\n        # project image features to the same space as text embeddings\n        image_features = self.project_image(image_features.transpose(1, 2).contiguous()).transpose(1, 2).contiguous()\n        image_features = self.transform_image_to_hidden(image_features)\n        image_features = self.image_align_attention(\n            image_features.transpose(0, 1), token_embeddings, token_embeddings)[0].transpose(0, 1).contiguous()\n\n        # concatenate image starts, image features, and image ends embeddings\n        image_inputs = torch.cat([torch.cat([image_starts, image_features], dim=1), image_ends], dim=1)\n\n        # concatenate image inputs to the original text embeddings\n        text_embeddings = torch.cat(\n            [torch.cat([text_embeddings[:, 0, :].unsqueeze(1), image_inputs], dim=1), \n            text_embeddings[:, 1:, :]], dim=1)\n\n        ingore_num += (image_inputs.size(1))\n\n    if 'attention_mask' in inputs:\n        # increase the length of attention mask by adding the length of multimodal inputs\n        attention_mask = torch.tensor([1]*ingore_num*text_embeddings.size(0), device=text_embeddings.device).view(text_embeddings.size(0), -1)  # (B X ignore_num)\n        attention_mask = torch.cat([attention_mask, inputs['attention_mask']], dim=1)\n    else:\n        attention_mask = None\n\n    if 'labels' in inputs and inputs['labels'] is not None:\n        # increase the length of labels by adding the length of labels\n        # we use -100 to ignore the loss of labels in multimodal inputs\n        # !!! we can replace -100 by config constants to make the code better\n\n        # since the tokens corresponding to the image_inputs, audio_inputs, and video_inputs are not part of the original text \n        # and don't have corresponding labels in the true text sequence, their labels are set to -100. This ensures that \n        # the model's predictions for these tokens don't affect the loss and, consequently, the gradients and the model's subsequent learning.\n        labels = torch.tensor([-100]*ingore_num*text_embeddings.size(0), device=text_embeddings.device).view(text_embeddings.size(0), -1)\n        labels = torch.cat([labels, inputs['labels']], dim=1)\n    else:\n        labels = None\n\n    # text_embeddings: (batch_size, sequence_length + ingore_num, embedding_dim)\n    # attention_mask: (batch_size, sequence_length + ingore_num). 1 denotes we should attend to, and 0 denotes we should not attend to the token.\n    # labels: (batch_size, sequence_length + ingore_num). -100 denotes we should ignore the token.\n    return text_embeddings, attention_mask, labels\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model_old.MM_LLMs_Config","title":"<code>MM_LLMs_Config</code>","text":"<p>             Bases: <code>PretrainedConfig</code></p> <p>This is the configuration class to store the configuration of a <code>MM_LLMsModel</code>. It contains class level and instance level attributes. It also contains the load (from_pretrained) and save (to_dict) methods for saving and loading configuration files.</p> Source code in <code>src/aeiva/model/macaw_model_old.py</code> <pre><code>class MM_LLMs_Config(PretrainedConfig):\n\"\"\"\n    This is the configuration class to store the configuration of a `MM_LLMsModel`.\n    It contains class level and instance level attributes.\n    It also contains the load (from_pretrained) and save (to_dict) methods for saving and loading configuration files.\n    \"\"\"\n    # general class attributes for all model instances\n    model_type = 'mm_llms'\n    is_composition = True\n\n    def __init__(self, n_frames=6, attention_heads=8, clip_config=None, whisper_config=None, llm_config=None, **kwargs):\n        self.image_config = clip_config\n        self.audio_config = whisper_config\n        self.llm_config = llm_config  # language model config\n        self.n_frames = n_frames  # video config information. How many frames are used for each video clip.\n        self.attention_heads = attention_heads\n        self.hidden_size = max(llm_config.hidden_size, clip_config.projection_dim, whisper_config.d_model, clip_config.projection_dim)\n        super().__init__(**kwargs)\n\n    def to_dict(self):\n\"\"\"\n        Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].\n        This method overrides the base class method to include serialization of the \n        image, audio, and language model configurations along with the base configuration.\n\n        Returns:\n            `Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n        \"\"\"\n        output = copy.deepcopy(self.__dict__)\n        output[\"image_config\"] = self.image_config.to_dict()\n        output[\"audio_config\"] = self.audio_config.to_dict()\n        output['llm_config'] = self.llm_config.to_dict()\n        output['n_frames'] = self.n_frames\n        output['attention_heads'] = self.attention_heads\n        output['hidden_size'] = self.hidden_size\n        output[\"model_type\"] = self.__class__.model_type\n        return output\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n\n        clip_config = CLIPConfig.from_dict(config_dict['image_config'])\n        whisper_config = WhisperConfig.from_dict(config_dict['audio_config'])\n        llm_config = LlamaConfig.from_dict(config_dict['llm_config'])\n\n        return cls(clip_config=clip_config, whisper_config=whisper_config, llm_config=llm_config, **kwargs)\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model_old.MM_LLMs_Config.to_dict","title":"<code>to_dict()</code>","text":"<p>Serializes this instance to a Python dictionary. Override the default [<code>~PretrainedConfig.to_dict</code>]. This method overrides the base class method to include serialization of the  image, audio, and language model configurations along with the base configuration.</p> <p>Returns:</p> Type Description <p><code>Dict[str, any]</code>: Dictionary of all the attributes that make up this configuration instance,</p> Source code in <code>src/aeiva/model/macaw_model_old.py</code> <pre><code>def to_dict(self):\n\"\"\"\n    Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].\n    This method overrides the base class method to include serialization of the \n    image, audio, and language model configurations along with the base configuration.\n\n    Returns:\n        `Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n    \"\"\"\n    output = copy.deepcopy(self.__dict__)\n    output[\"image_config\"] = self.image_config.to_dict()\n    output[\"audio_config\"] = self.audio_config.to_dict()\n    output['llm_config'] = self.llm_config.to_dict()\n    output['n_frames'] = self.n_frames\n    output['attention_heads'] = self.attention_heads\n    output['hidden_size'] = self.hidden_size\n    output[\"model_type\"] = self.__class__.model_type\n    return output\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model_old.WhisperEncoder","title":"<code>WhisperEncoder</code>","text":"<p>             Bases: <code>WhisperPreTrainedModel</code></p> <p>Transformer encoder consisting of config.encoder_layers self attention layers. Each layer is a [<code>WhisperEncoderLayer</code>].</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>WhisperConfig</code> <p>WhisperConfig</p> required Source code in <code>src/aeiva/model/macaw_model_old.py</code> <pre><code>class WhisperEncoder(WhisperPreTrainedModel):\n\"\"\"\n    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n    [`WhisperEncoderLayer`].\n\n    Args:\n        config: WhisperConfig\n    \"\"\"\n\n    def __init__(self, config: WhisperConfig):\n        super().__init__(config)\n        self.dropout = config.dropout\n        self.layerdrop = config.encoder_layerdrop\n\n        embed_dim = config.d_model\n        # num_mel_bins corresponds to the number of features extracted from the audio signal for each time step. \n        # When we convert audio to a Mel spectrogram, each time step (or frame) in the spectrogram \n        # is represented by a feature vector of size num_mel_bins. \n        self.num_mel_bins = config.num_mel_bins\n        self.padding_idx = config.pad_token_id\n        self.max_source_positions = config.max_source_positions\n        # embed_scale is a scaling factor that is applied to the embeddings.\n        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n\n        self.conv1 = nn.Conv1d(self.num_mel_bins, embed_dim, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=2, padding=1)\n\n        # position embedding layer\n        self.embed_positions = nn.Embedding(self.max_source_positions, embed_dim)\n\n        self.layers = nn.ModuleList([WhisperEncoderLayer(config) for _ in range(config.encoder_layers)])\n        self.layer_norm = nn.LayerNorm(config.d_model)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def _freeze_parameters(self):\n        for param in self.parameters():\n            param.requires_grad = False\n        self._requires_grad = False\n\n    def get_input_embeddings(self) -&gt; nn.Module:\n        return self.conv1\n\n    def set_input_embeddings(self, value: nn.Module):\n        self.conv1 = value\n\n    def forward(\n        self,\n        input_features,\n        attention_mask=None,\n        head_mask=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\nr\"\"\"\n        Args:\n            input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n                Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n                obtained by loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a\n                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n                `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n                and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n            attention_mask (`torch.Tensor`)`, *optional*):\n                Whisper does not support masking of the `input_features`, this argument is preserved for compatibility,\n                but it is not used. By default the silence in the input log mel spectrogram are ignored.\n            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        # set output flags\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # embed audio features\n        # input_features shape: (batch_size, feature_size, sequence_length)\n        inputs_embeds = nn.functional.gelu(self.conv1(input_features))  # (batch_size, embed_dim, sequence_length)\n        inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))  # (batch_size, embed_dim, sequence_length/2), because the stride is 2. Downsampling by 2.\n        inputs_embeds = inputs_embeds.permute(0, 2, 1)  #  (batch_size, sequence_length/2, embed_dim)\n        embed_pos = self.embed_positions.weight  # (max_source_positions, embed_dim)\n\n        # add position embedding to audio features embedding\n        # !!!: Do max_source_positions and sequence_length/2 must be the same??? Kind of confusing.\n        hidden_states = inputs_embeds + embed_pos\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\n        encoder_states = () if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n\n        # check if head_mask has a correct number of layers specified if desired\n        if head_mask is not None:\n            assert head_mask.size()[0] == (\n                len(self.layers)\n            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n\n        # go through the whisper encoder layers to get the hidden states and attentions in all layers\n        for idx, encoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                encoder_states = encoder_states + (hidden_states,)\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            dropout_probability = random.uniform(0, 1)\n            if self.training and (dropout_probability &lt; self.layerdrop):  # skip the layer\n                layer_outputs = (None, None)\n            else:\n                if self.gradient_checkpointing and self.training:\n\n                    def create_custom_forward(module):\n                        def custom_forward(*inputs):\n                            return module(*inputs, output_attentions)\n\n                        return custom_forward\n\n                    layer_outputs = torch.utils.checkpoint.checkpoint(\n                        create_custom_forward(encoder_layer),\n                        hidden_states,\n                        None,\n                        (head_mask[idx] if head_mask is not None else None),\n                    )\n                else:\n                    # The layer_outputs is a tuple of (hidden_states, attention).\n                    # The attention is None if output_attentions is False.\n                    # hidden_states shape: (batch_size, sequence_length/2, embed_dim), as stride is 2 in the self.conv2\n                    # attention shape: (batch_size, num_heads, sequence_length/2, sequence_length/2)\n                    layer_outputs = encoder_layer(\n                        hidden_states,\n                        None,\n                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                        output_attentions=output_attentions,\n                    )\n\n                hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n\n        hidden_states = self.layer_norm(hidden_states)\n        if output_hidden_states:\n            encoder_states = encoder_states + (hidden_states,)\n\n        # output\n        if not return_dict:\n            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n        return BaseModelOutput(\n            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n        )\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model_old.WhisperEncoder.forward","title":"<code>forward(input_features, attention_mask=None, head_mask=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_features</code> <code>`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`</code> <p>Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by loading a <code>.flac</code> or <code>.wav</code> audio file into an array of type <code>List[float]</code> or a <code>numpy.ndarray</code>, e.g. via the soundfile library (<code>pip install soundfile</code>). To prepare the array into <code>input_features</code>, the [<code>AutoFeatureExtractor</code>] should be used for extracting the mel features, padding and conversion into a tensor of type <code>torch.FloatTensor</code>. See [<code>~WhisperFeatureExtractor.__call__</code>]</p> required <code>attention_mask</code> <code>`torch.Tensor`)`, *optional*</code> <p>Whisper does not support masking of the <code>input_features</code>, this argument is preserved for compatibility, but it is not used. By default the silence in the input log mel spectrogram are ignored.</p> <code>None</code> <code>head_mask</code> <code>`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*</code> <p>Mask to nullify selected heads of the attention modules. Mask values selected in <code>[0, 1]</code>:</p> <ul> <li>1 indicates the head is not masked,</li> <li>0 indicates the head is masked.</li> </ul> <code>None</code> <code>output_attentions</code> <code>`bool`, *optional*</code> <p>Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned tensors for more detail.</p> <code>None</code> <code>output_hidden_states</code> <code>`bool`, *optional*</code> <p>Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for more detail.</p> <code>None</code> <code>return_dict</code> <code>`bool`, *optional*</code> <p>Whether or not to return a [<code>~utils.ModelOutput</code>] instead of a plain tuple.</p> <code>None</code> Source code in <code>src/aeiva/model/macaw_model_old.py</code> <pre><code>def forward(\n    self,\n    input_features,\n    attention_mask=None,\n    head_mask=None,\n    output_attentions=None,\n    output_hidden_states=None,\n    return_dict=None,\n):\nr\"\"\"\n    Args:\n        input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n            Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n            obtained by loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a\n            `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n            `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n            and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n        attention_mask (`torch.Tensor`)`, *optional*):\n            Whisper does not support masking of the `input_features`, this argument is preserved for compatibility,\n            but it is not used. By default the silence in the input log mel spectrogram are ignored.\n        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n            Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n            returned tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n            for more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n    \"\"\"\n    # set output flags\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    # embed audio features\n    # input_features shape: (batch_size, feature_size, sequence_length)\n    inputs_embeds = nn.functional.gelu(self.conv1(input_features))  # (batch_size, embed_dim, sequence_length)\n    inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))  # (batch_size, embed_dim, sequence_length/2), because the stride is 2. Downsampling by 2.\n    inputs_embeds = inputs_embeds.permute(0, 2, 1)  #  (batch_size, sequence_length/2, embed_dim)\n    embed_pos = self.embed_positions.weight  # (max_source_positions, embed_dim)\n\n    # add position embedding to audio features embedding\n    # !!!: Do max_source_positions and sequence_length/2 must be the same??? Kind of confusing.\n    hidden_states = inputs_embeds + embed_pos\n    hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\n    encoder_states = () if output_hidden_states else None\n    all_attentions = () if output_attentions else None\n\n    # check if head_mask has a correct number of layers specified if desired\n    if head_mask is not None:\n        assert head_mask.size()[0] == (\n            len(self.layers)\n        ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n\n    # go through the whisper encoder layers to get the hidden states and attentions in all layers\n    for idx, encoder_layer in enumerate(self.layers):\n        if output_hidden_states:\n            encoder_states = encoder_states + (hidden_states,)\n        # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n        dropout_probability = random.uniform(0, 1)\n        if self.training and (dropout_probability &lt; self.layerdrop):  # skip the layer\n            layer_outputs = (None, None)\n        else:\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(*inputs, output_attentions)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(encoder_layer),\n                    hidden_states,\n                    None,\n                    (head_mask[idx] if head_mask is not None else None),\n                )\n            else:\n                # The layer_outputs is a tuple of (hidden_states, attention).\n                # The attention is None if output_attentions is False.\n                # hidden_states shape: (batch_size, sequence_length/2, embed_dim), as stride is 2 in the self.conv2\n                # attention shape: (batch_size, num_heads, sequence_length/2, sequence_length/2)\n                layer_outputs = encoder_layer(\n                    hidden_states,\n                    None,\n                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n                    output_attentions=output_attentions,\n                )\n\n            hidden_states = layer_outputs[0]\n\n        if output_attentions:\n            all_attentions = all_attentions + (layer_outputs[1],)\n\n    hidden_states = self.layer_norm(hidden_states)\n    if output_hidden_states:\n        encoder_states = encoder_states + (hidden_states,)\n\n    # output\n    if not return_dict:\n        return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n    return BaseModelOutput(\n        last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n    )\n</code></pre>"},{"location":"reference/#src.aeiva.model.macaw_model_old.rotate_half","title":"<code>rotate_half(x)</code>","text":"<p>Rotates half the hidden dims of the input.</p> Source code in <code>src/aeiva/model/macaw_model_old.py</code> <pre><code>def rotate_half(x):\n\"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n</code></pre>"},{"location":"reference/#src.aeiva.operator","title":"<code>operator</code>","text":""},{"location":"reference/#src.aeiva.operator.custom_ops","title":"<code>custom_ops</code>","text":""},{"location":"reference/#src.aeiva.operator.custom_ops.macaw_dataitem_ops","title":"<code>macaw_dataitem_ops</code>","text":"<p>This module contains the data item processing functions.</p> <p>For a data item processing function, it takes a data example (a dict) as input and return a processed data example.</p> <p>@Author: Bang Liu (chatsci.ai@gmail.com) @Date: 2023-07-11</p> <p>Copyright (C) 2023 Bang Liu - All Rights Reserved. This source code is licensed under the license found in the LICENSE file in the root directory of this source tree.</p>"},{"location":"reference/#src.aeiva.operator.dataitem_ops","title":"<code>dataitem_ops</code>","text":"<p>This module contains the data item processing functions.</p> <p>For a data item processing function, it takes a data example (a dict) as input and return a processed data example.</p> <p>@Author: Bang Liu (chatsci.ai@gmail.com) @Date: 2023-07-11</p> <p>Copyright (C) 2023 Bang Liu - All Rights Reserved. This source code is licensed under the license found in the LICENSE file in the root directory of this source tree.</p>"},{"location":"reference/#src.aeiva.operator.dataset_ops","title":"<code>dataset_ops</code>","text":"<p>This module contains the utils for processing datasets.</p> <p>A dataset in aeiva is a dictionary with the following structure: {     \"data\": [         {sample1}, {sample2}, ..., {sampleN}     ],     \"metadata\": {         \"num_samples\": XX,          ...     } } where each sample is a dictionary itself, and metadata is a dictionary that contains the number of samples and possibly other fields.</p> <p>@Author: Bang Liu (chatsci.ai@gmail.com) @Date: 2023-07-13</p> <p>Copyright (C) 2023 Bang Liu - All Rights Reserved. This source code is licensed under the license found in the LICENSE file in the root directory of this source tree.</p>"},{"location":"reference/#src.aeiva.operator.dataset_ops.build_and_merge_datasets","title":"<code>build_and_merge_datasets(dataset_names, input_filepaths_dict, pipeline, output_dir, max_samples=sys.maxsize)</code>","text":"<p>Build multiple datasets by formatting and processing them.</p> Source code in <code>src/aeiva/operator/dataset_ops.py</code> <pre><code>def build_and_merge_datasets(dataset_names: list[str],\n                             input_filepaths_dict: dict[str, str],\n                             pipeline: list[Callable],\n                             output_dir: Optional[str],\n                             max_samples: Optional[int] = sys.maxsize) -&gt; DataSet:\nr\"\"\" Build multiple datasets by formatting and processing them.\n    \"\"\"\n    merged_datasets = []\n    for dataset_name in dataset_names:\n        dataset = build_dataset(dataset_name, input_filepaths_dict, pipeline, output_dir, max_samples)\n        merged_datasets.append(dataset)\n    result = merge_datasets(merged_datasets)\n    return result\n</code></pre>"},{"location":"reference/#src.aeiva.operator.dataset_ops.build_dataset","title":"<code>build_dataset(dataset_name, input_filepaths_dict, pipeline, output_dir, max_samples=sys.maxsize)</code>","text":"<p>Build a dataset by formatting and processing it.</p> Source code in <code>src/aeiva/operator/dataset_ops.py</code> <pre><code>def build_dataset(dataset_name: str,\n                  input_filepaths_dict: dict[str, str],\n                  pipeline: list[Callable],\n                  output_dir: Optional[str],\n                  max_samples: Optional[int] = sys.maxsize) -&gt; DataSet:\nr\"\"\" Build a dataset by formatting and processing it.\n    \"\"\"\n    operator_type = 'data_formatter'\n    format_func = OPERATORS[operator_type][dataset_name]\n    formatted_dataset = format_func(input_filepaths_dict, output_dir, max_samples)\n    processed_dataset = process_dataset(formatted_dataset, pipeline, output_dir, dataset_name)\n    print(f\"Completed processing dataset: {dataset_name} (output_dir: {output_dir})\")\n    return processed_dataset\n</code></pre>"},{"location":"reference/#src.aeiva.operator.dataset_ops.filter_dataset","title":"<code>filter_dataset(dataset, filter_criteria, *args, **kwargs)</code>","text":"<p>Filter a dataset by a filter function.</p> Source code in <code>src/aeiva/operator/dataset_ops.py</code> <pre><code>def filter_dataset(dataset: DataSet, filter_criteria: str, *args, **kwargs) -&gt; DataSet:\nr\"\"\" Filter a dataset by a filter function.\n    \"\"\"\n    operator_type = 'data_filter'\n    filter_func = OPERATORS[operator_type][filter_criteria]\n    filtered_data = filter_func(dataset, *args, **kwargs)\n    return filtered_data\n</code></pre>"},{"location":"reference/#src.aeiva.operator.dataset_ops.filter_dataset_by_keys","title":"<code>filter_dataset_by_keys(dataset, keys_to_preserve)</code>","text":"<p>Filter the dataset to only include specified keys in each sample.</p> Source code in <code>src/aeiva/operator/dataset_ops.py</code> <pre><code>@register_data_filter(\"filter_dataset_by_keys\")\ndef filter_dataset_by_keys(dataset: DataSet, keys_to_preserve: list[str]) -&gt; DataSet:\nr\"\"\" Filter the dataset to only include specified keys in each sample.\n    \"\"\"\n    filtered_data = []\n    for sample in dataset[\"data\"]:\n        for key in keys_to_preserve:\n            if key not in sample:\n                raise KeyError(f\"Key {key} not found in sample\")\n        filtered_sample = {key: sample[key] for key in keys_to_preserve if key in sample}\n        filtered_data.append(filtered_sample)\n    return {\"data\": filtered_data, \"metadata\": dataset[\"metadata\"]}\n</code></pre>"},{"location":"reference/#src.aeiva.operator.dataset_ops.merge_datasets","title":"<code>merge_datasets(datasets)</code>","text":"<p>Merge multiple datasets into one.</p> Source code in <code>src/aeiva/operator/dataset_ops.py</code> <pre><code>def merge_datasets(datasets: list[DataSet]) -&gt; DataSet:\nr\"\"\" Merge multiple datasets into one.\n    \"\"\"\n    merged_data = []\n    total_samples = 0\n    for dataset in datasets:\n        merged_data.extend(dataset[\"data\"])\n        total_samples += dataset[\"metadata\"][\"num_samples\"]\n    result = {\"data\": merged_data, \"metadata\": {\"num_samples\": total_samples}}\n    return result\n</code></pre>"},{"location":"reference/#src.aeiva.operator.dataset_ops.sample_dataset","title":"<code>sample_dataset(dataset, n_samples)</code>","text":"<p>Sample a number of samples from a dataset.</p> Source code in <code>src/aeiva/operator/dataset_ops.py</code> <pre><code>def sample_dataset(dataset: DataSet, n_samples: int) -&gt; DataSet:\nr\"\"\" Sample a number of samples from a dataset.\n    \"\"\"\n    random_indices = random.sample(range(dataset[\"metadata\"][\"num_samples\"]), n_samples)\n    sampled_data = [dataset[\"data\"][i] for i in random_indices]\n    return {\"data\": sampled_data, \"metadata\": {\"num_samples\": n_samples}}\n</code></pre>"},{"location":"reference/#src.aeiva.operator.dataset_ops.save_dataset","title":"<code>save_dataset(dataset, output_path)</code>","text":"<p>Save a dataset to a file by pickling it.</p> Source code in <code>src/aeiva/operator/dataset_ops.py</code> <pre><code>def save_dataset(dataset: DataSet, output_path: str) -&gt; None:\nr\"\"\" Save a dataset to a file by pickling it.\n    \"\"\"\n    ensure_dir(output_path)\n    pickle.dump(dataset, open(output_path, \"wb\"), protocol=4)\n</code></pre>"},{"location":"reference/#src.aeiva.operator.dataset_ops.split_dataset","title":"<code>split_dataset(dataset, train_ratio, seed=42)</code>","text":"<p>Split a dataset into a training set and a validation set.</p> Source code in <code>src/aeiva/operator/dataset_ops.py</code> <pre><code>def split_dataset(dataset: dict, train_ratio: float, seed: int = 42) -&gt; Tuple[dict]:\nr\"\"\" Split a dataset into a training set and a validation set.\n    \"\"\"\n    np.random.seed(seed)  # ensures the function is deterministic\n\n    data = dataset[\"data\"]\n    metadata = dataset[\"metadata\"]\n\n    # Create a permutation of indices and shuffle the data.\n    perm = np.random.permutation(len(data))\n    shuffled_data = [data[i] for i in perm]\n\n    # Calculate split index\n    split_idx = int(train_ratio * len(shuffled_data))\n\n    # Split the shuffled data\n    train_data = shuffled_data[:split_idx]\n    val_data = shuffled_data[split_idx:]\n\n    # Create metadata for training and validation datasets\n    train_metadata = metadata.copy()\n    train_metadata[\"num_samples\"] = len(train_data)\n    val_metadata = metadata.copy()\n    val_metadata[\"num_samples\"] = len(val_data)\n\n    # Create training and validation datasets\n    train_dataset = {\"data\": train_data, \"metadata\": train_metadata}\n    val_dataset = {\"data\": val_data, \"metadata\": val_metadata}\n\n    return train_dataset, val_dataset\n</code></pre>"},{"location":"reference/#src.aeiva.trainer","title":"<code>trainer</code>","text":""},{"location":"reference/#src.aeiva.trainer.pl_trainer","title":"<code>pl_trainer</code>","text":""},{"location":"reference/#src.aeiva.trainer.pl_trainer.LightningTrainer","title":"<code>LightningTrainer</code>","text":"<p>             Bases: <code>pl.LightningModule</code></p> Source code in <code>src/aeiva/trainer/pl_trainer.py</code> <pre><code>class LightningTrainer(pl.LightningModule):\n    def __init__(self, model, tokenizer, config):\n        super().__init__()\n        self.model = model\n        self.tokenizer = tokenizer\n        self.config = config\n\n    def forward(self, batch):\n        outputs = self.model(batch)\n        return outputs\n\n    def training_step(self, batch, batch_idx):\n        outputs = self(batch)\n        loss = outputs.loss\n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_idx):\n        outputs = self(batch)\n        loss = outputs.loss\n        return {\"loss\": loss}\n\n    def test_step(self, batch, batch_idx):\n        outputs = self(batch)\n        loss = outputs.loss\n        return {\"loss\": loss}\n\n    def configure_optimizers(self):\n\"\"\"\n        Function to prepare the optimizer and learning rate scheduler for model training.\n        This function separates model parameters into two categories: parameters that will experience weight decay, \n        and those that will not (e.g., bias and layernorm weights). \n\n        Returns:\n            Tuple[Optimizer, Scheduler]: Tuple containing the optimizer and learning rate scheduler.\n        \"\"\"\n\n        # List of module types that will be subjected to weight decay.\n        whitelist_weight_modules = (torch.nn.Linear, ) \n\n        # List of module types that will not be subjected to weight decay.\n        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n\n        # Parameter sets for decay and no decay.\n        decay = set()\n        no_decay = set()\n\n        # Populate the decay and no_decay sets. \n        # Loop over all modules to get module name (mn) and module (m).\n        # !!!! revise later.\n        # for mn, m in self.model.named_modules():\n        #     for pn, p in m.named_parameters():\n        #         fpn = '%s.%s' % (mn, pn) if mn else pn \n\n        #         if 'bias' in pn:\n        #             no_decay.add(fpn)\n        #         elif 'weight' in pn:\n        #             decay.add(fpn)\n\n        param_dict = {pn: p for pn, p in self.model.named_parameters()}\n\n        for mn, m in self.model.named_modules():\n            for pn, p in m.named_parameters():\n                fpn = '%s.%s' % (mn, pn) if mn else pn  # full param name\n                # random note: because named_modules and named_parameters are recursive\n                # we will see the same tensors p many many times. but doing it this way\n                # allows us to know which parent module any tensor p belongs to...\n                # Adding new condition to check for the 'class_embedding' and 'logit_scale' parameters\n                if pn.endswith('bias') or 'class_embedding' in pn or 'logit_scale' in pn:\n                    no_decay.add(fpn)\n                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n                    no_decay.add(fpn)\n        for pn, p in param_dict.items():\n            if pn not in no_decay:\n                decay.add(pn)\n\n\n        # # After this loop, print out all parameters in the intersection of decay and no_decay:\n        # print(\"decay: \", decay)\n        # print(\"no_decay: \", no_decay)\n        # print(\"intersection: \", decay.intersection(no_decay))\n\n        # print(\"difference: \", param_dict.keys() - (decay | no_decay))\n\n\n        # # 'lm_head.weight' is tied to 'model.embed_tokens.weight', so it should not be decayed. \n        # # This ensures that the same tensor is not optimized in different ways.\n        # decay.remove('llm.lm_head.weight')\n\n        # Validate that we considered every parameter.\n        param_dict = {pn: p for pn, p in self.model.named_parameters()}\n        assert len(decay &amp; no_decay) == 0, \"Some parameters are in both decay and no_decay sets!\"\n        assert len(param_dict.keys() - (decay | no_decay)) == 0, \"Some parameters are in neither decay nor no_decay sets!\"\n\n        # Create the PyTorch optimizer object.\n        optim_groups = [\n            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": self.config.weight_decay},\n            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n        ]\n        # new PyTorch nightly has a new 'fused' option for AdamW that is much faster\n        use_fused = (self.config.device == 'cuda') and (\n            'fused' in inspect.signature(torch.optim.AdamW).parameters)\n        print(f\"using fused AdamW: {use_fused}\")\n        extra_args = dict(fused=True) if use_fused else dict()\n        optimizer = torch.optim.AdamW(\n            optim_groups, lr=self.config.learning_rate, betas=(self.config.adam_beta1, self.config.adam_beta2), **extra_args)\n\n        # Prepare learning rate scheduler.\n        total_steps = self.config.max_steps\n        pct_start = self.config.warmup_steps / total_steps\n        final_div_factor = self.config.learning_rate / self.config.min_lr\n\n        scheduler = {\n            'scheduler': torch.optim.lr_scheduler.OneCycleLR(\n                optimizer,\n                max_lr=self.config.learning_rate,\n                total_steps=total_steps,\n                pct_start=pct_start,\n                final_div_factor=final_div_factor,\n                div_factor=1.0,  # No additional scaling for the initial learning rate\n                anneal_strategy='cos',  # Use cosine annealing\n                cycle_momentum=False,  # Disable momentum cycling\n            ),\n            'interval': 'step',\n            'frequency': 1\n        }\n\n        return [optimizer], [scheduler]\n\n\n    def get_num_params(self, non_embedding=True):\n\"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.model.parameters())\n        if non_embedding:\n            embedding_params = sum(p.numel() for m in self.model.modules() if isinstance(m, nn.Embedding) for p in m.parameters())\n            n_params -= embedding_params\n        return n_params\n\n    def estimate_mfu(self, fwdbwd_per_iter, dt):\n\"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n        # first estimate the number of flops we do per iteration.\n        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n        N = self.get_num_params()\n        cfg = self.config\n        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n        flops_per_token = 6*N + 12*L*H*Q*T\n        flops_per_fwdbwd = flops_per_token * T\n        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n        # express our flops throughput as ratio of A100 bfloat16 peak flops\n        flops_achieved = flops_per_iter * (1.0/dt)  # per second\n        flops_promised = 312e12  # A100 GPU bfloat16 peak flops is 312 TFLOPS\n        mfu = flops_achieved / flops_promised\n        return mfu\n</code></pre>"},{"location":"reference/#src.aeiva.trainer.pl_trainer.LightningTrainer.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Function to prepare the optimizer and learning rate scheduler for model training. This function separates model parameters into two categories: parameters that will experience weight decay,  and those that will not (e.g., bias and layernorm weights). </p> <p>Returns:</p> Type Description <p>Tuple[Optimizer, Scheduler]: Tuple containing the optimizer and learning rate scheduler.</p> Source code in <code>src/aeiva/trainer/pl_trainer.py</code> <pre><code>def configure_optimizers(self):\n\"\"\"\n    Function to prepare the optimizer and learning rate scheduler for model training.\n    This function separates model parameters into two categories: parameters that will experience weight decay, \n    and those that will not (e.g., bias and layernorm weights). \n\n    Returns:\n        Tuple[Optimizer, Scheduler]: Tuple containing the optimizer and learning rate scheduler.\n    \"\"\"\n\n    # List of module types that will be subjected to weight decay.\n    whitelist_weight_modules = (torch.nn.Linear, ) \n\n    # List of module types that will not be subjected to weight decay.\n    blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n\n    # Parameter sets for decay and no decay.\n    decay = set()\n    no_decay = set()\n\n    # Populate the decay and no_decay sets. \n    # Loop over all modules to get module name (mn) and module (m).\n    # !!!! revise later.\n    # for mn, m in self.model.named_modules():\n    #     for pn, p in m.named_parameters():\n    #         fpn = '%s.%s' % (mn, pn) if mn else pn \n\n    #         if 'bias' in pn:\n    #             no_decay.add(fpn)\n    #         elif 'weight' in pn:\n    #             decay.add(fpn)\n\n    param_dict = {pn: p for pn, p in self.model.named_parameters()}\n\n    for mn, m in self.model.named_modules():\n        for pn, p in m.named_parameters():\n            fpn = '%s.%s' % (mn, pn) if mn else pn  # full param name\n            # random note: because named_modules and named_parameters are recursive\n            # we will see the same tensors p many many times. but doing it this way\n            # allows us to know which parent module any tensor p belongs to...\n            # Adding new condition to check for the 'class_embedding' and 'logit_scale' parameters\n            if pn.endswith('bias') or 'class_embedding' in pn or 'logit_scale' in pn:\n                no_decay.add(fpn)\n            elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n                no_decay.add(fpn)\n    for pn, p in param_dict.items():\n        if pn not in no_decay:\n            decay.add(pn)\n\n\n    # # After this loop, print out all parameters in the intersection of decay and no_decay:\n    # print(\"decay: \", decay)\n    # print(\"no_decay: \", no_decay)\n    # print(\"intersection: \", decay.intersection(no_decay))\n\n    # print(\"difference: \", param_dict.keys() - (decay | no_decay))\n\n\n    # # 'lm_head.weight' is tied to 'model.embed_tokens.weight', so it should not be decayed. \n    # # This ensures that the same tensor is not optimized in different ways.\n    # decay.remove('llm.lm_head.weight')\n\n    # Validate that we considered every parameter.\n    param_dict = {pn: p for pn, p in self.model.named_parameters()}\n    assert len(decay &amp; no_decay) == 0, \"Some parameters are in both decay and no_decay sets!\"\n    assert len(param_dict.keys() - (decay | no_decay)) == 0, \"Some parameters are in neither decay nor no_decay sets!\"\n\n    # Create the PyTorch optimizer object.\n    optim_groups = [\n        {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": self.config.weight_decay},\n        {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n    ]\n    # new PyTorch nightly has a new 'fused' option for AdamW that is much faster\n    use_fused = (self.config.device == 'cuda') and (\n        'fused' in inspect.signature(torch.optim.AdamW).parameters)\n    print(f\"using fused AdamW: {use_fused}\")\n    extra_args = dict(fused=True) if use_fused else dict()\n    optimizer = torch.optim.AdamW(\n        optim_groups, lr=self.config.learning_rate, betas=(self.config.adam_beta1, self.config.adam_beta2), **extra_args)\n\n    # Prepare learning rate scheduler.\n    total_steps = self.config.max_steps\n    pct_start = self.config.warmup_steps / total_steps\n    final_div_factor = self.config.learning_rate / self.config.min_lr\n\n    scheduler = {\n        'scheduler': torch.optim.lr_scheduler.OneCycleLR(\n            optimizer,\n            max_lr=self.config.learning_rate,\n            total_steps=total_steps,\n            pct_start=pct_start,\n            final_div_factor=final_div_factor,\n            div_factor=1.0,  # No additional scaling for the initial learning rate\n            anneal_strategy='cos',  # Use cosine annealing\n            cycle_momentum=False,  # Disable momentum cycling\n        ),\n        'interval': 'step',\n        'frequency': 1\n    }\n\n    return [optimizer], [scheduler]\n</code></pre>"},{"location":"reference/#src.aeiva.trainer.pl_trainer.LightningTrainer.estimate_mfu","title":"<code>estimate_mfu(fwdbwd_per_iter, dt)</code>","text":"<p>estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS</p> Source code in <code>src/aeiva/trainer/pl_trainer.py</code> <pre><code>def estimate_mfu(self, fwdbwd_per_iter, dt):\n\"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n    # first estimate the number of flops we do per iteration.\n    # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n    N = self.get_num_params()\n    cfg = self.config\n    L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n    flops_per_token = 6*N + 12*L*H*Q*T\n    flops_per_fwdbwd = flops_per_token * T\n    flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n    # express our flops throughput as ratio of A100 bfloat16 peak flops\n    flops_achieved = flops_per_iter * (1.0/dt)  # per second\n    flops_promised = 312e12  # A100 GPU bfloat16 peak flops is 312 TFLOPS\n    mfu = flops_achieved / flops_promised\n    return mfu\n</code></pre>"},{"location":"reference/#src.aeiva.trainer.pl_trainer.LightningTrainer.get_num_params","title":"<code>get_num_params(non_embedding=True)</code>","text":"<p>Return the number of parameters in the model. For non-embedding count (default), the position embeddings get subtracted. The token embeddings would too, except due to the parameter sharing these params are actually used as weights in the final layer, so we include them.</p> Source code in <code>src/aeiva/trainer/pl_trainer.py</code> <pre><code>def get_num_params(self, non_embedding=True):\n\"\"\"\n    Return the number of parameters in the model.\n    For non-embedding count (default), the position embeddings get subtracted.\n    The token embeddings would too, except due to the parameter sharing these\n    params are actually used as weights in the final layer, so we include them.\n    \"\"\"\n    n_params = sum(p.numel() for p in self.model.parameters())\n    if non_embedding:\n        embedding_params = sum(p.numel() for m in self.model.modules() if isinstance(m, nn.Embedding) for p in m.parameters())\n        n_params -= embedding_params\n    return n_params\n</code></pre>"},{"location":"reference/#src.aeiva.util","title":"<code>util</code>","text":""},{"location":"reference/#src.aeiva.util.token_utils","title":"<code>token_utils</code>","text":""},{"location":"reference/#src.aeiva.util.token_utils.pad_or_truncate_tokens","title":"<code>pad_or_truncate_tokens(tokens, max_length, pad_token_id)</code>","text":"<p>This function aims to pad or truncate tokens to max_length.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list</code> <p>the list of tokens.</p> required <code>max_length</code> <code>int</code> <p>the max length of tokens.</p> required <code>pad_token_id</code> <code>int</code> <p>the id of pad token.</p> required <p>Returns:</p> Name Type Description <code>tokens</code> <code>list</code> <p>the list of tokens after padding or truncating.</p> Source code in <code>src/aeiva/util/token_utils.py</code> <pre><code>def pad_or_truncate_tokens(tokens, max_length, pad_token_id):\n\"\"\" This function aims to pad or truncate tokens to max_length.\n\n    Args:\n        tokens (list): the list of tokens.\n        max_length (int): the max length of tokens.\n        pad_token_id (int): the id of pad token.\n\n    Returns:\n        tokens (list): the list of tokens after padding or truncating.\n    \"\"\"\n    if len(tokens) &gt; max_length:\n        tokens = tokens[:max_length]\n    elif len(tokens) &lt; max_length:\n        tokens = tokens + [pad_token_id] * (max_length - len(tokens))\n    return tokens\n</code></pre>"},{"location":"roadmp/","title":"Roadmap","text":"<p>In this document, we describe the long- and short-term objectives of this project, as well as show the current plan in achieving the objectives.</p>"},{"location":"roadmp/#long-term-objective","title":"Long-term Objective","text":"<p>Enable Aeiva to learn from multimodal and embodied environments with high learning efficiency and low-resource requirements. Specifically, we aim to research on the following aspects:</p> <ul> <li> <p>Multimodal learning</p> <ul> <li>Be able to learn from video data, which is the main modality for human beings.</li> <li>Be able to intergrate different modalities, e.g., text, image, audio, video, documents, and so on.</li> </ul> </li> <li> <p>Embodied learning</p> <ul> <li>Be able to learn from embodied environments, e.g., MineDojo or any other video games, without reading internal states (i.e., by just observing raw visual inputs).</li> <li>Be able to imagine the environment of videos and learn from them.</li> <li>Unify the learning paradigm in different environments and different modalities.</li> </ul> </li> <li> <p>AI for Science</p> <ul> <li>Health</li> <li>Material Science</li> </ul> </li> <li> <p>Safe, Controllable, Interpretable AI</p> <ul> <li>Research on techniques to ensure safe, controllable, and interpretable AI</li> </ul> </li> <li> <p>Efficient learning</p> <ul> <li>Understand how LLMs learn with large-scale data</li> <li>Improve the learning efficiency of language models.</li> <li>Reducing the model size while maintaining the capability of models.</li> <li>The final goal is approaching or even surpassing the learning and energy efficiency of human brain.</li> </ul> </li> <li> <p>Artificial Neural Science</p> <ul> <li>Understand LLMs and other DL models: what they learned, how they learn, and how to improve.</li> <li>Intergrating neural science knowledge to improve DL models.</li> </ul> </li> <li> <p>Manipulating tools</p> <ul> <li>Enable the models to utilize tools, quickly learn new tools, and interact with different environments with tools</li> </ul> </li> <li> <p>AI Society</p> <ul> <li>Evolving a society of AI agents</li> <li>Combining AI agents with virtual environment</li> <li>Agents learn to solve problems in real-world</li> <li>Ensure the human rights</li> </ul> </li> </ul>"},{"location":"roadmp/#short-term-objective-keep-updating","title":"Short-term objective (keep updating)","text":"<p>The long-term objective of this project is quite ambitious. At the first place, we want to better understand how the current LLMs learn, and improve the multimodal and embodied learning. Specifically, we want to learn from videos efficiently. Below is a list of milestones we aim to achieve in a short-term (keep updating):</p> <ul> <li>Multimodal Learning<ul> <li>Benchmarking several existing multimodal LLMs in a unified manner</li> <li>Making model construction as easy as playing LEGO</li> <li>Unifing different datasets</li> <li>Define the general framework of multimodal learning</li> </ul> </li> <li>Embodied Learning<ul> <li>Integrating several embodied environments, e.g., MineDojo, Alfred, etc.</li> <li>Completing the framework of embodied learning, i.e., equiping LLMs with environments, actions, rewards, goals, and so on.</li> </ul> </li> <li>AI Agent<ul> <li>Design agent and related classes</li> <li>Design memory module</li> <li>Design world model module</li> <li>Design actions, rewards, etc.</li> <li>Implement several learning algorithms.</li> </ul> </li> <li>AI Society<ul> <li>Design AI community and communication protocals</li> <li>Add visualizatioin UI</li> </ul> </li> </ul> <p>......</p>"},{"location":"tutorials/","title":"Tutorials","text":""}]}